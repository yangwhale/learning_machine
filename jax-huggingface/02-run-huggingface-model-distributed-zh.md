# å¦‚ä½•åœ¨ JAX ä¸­è¿è¡Œ Hugging Face æ¨¡å‹ï¼ˆç¬¬äºŒéƒ¨åˆ†ï¼‰

åœ¨[ä¸Šä¸€ç¯‡æ–‡ç« ](01-run-huggingface-model-in-jax-zh.md)ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†å¦‚ä½•ä½¿ç”¨ Hugging Face å’Œ JAX å¯¹ Llama æ¨¡å‹æ‰§è¡Œ**å‰å‘ä¼ æ’­**ã€‚è¿™æ¬¡ï¼Œæˆ‘ä»¬å°†å®ç°ç›¸åŒçš„ç›®æ ‡ï¼Œä½†æ˜¯é€šè¿‡**åŒæ—¶ä½¿ç”¨å…«ä¸ªè®¾å¤‡**ã€‚

-----

## å¼ é‡å¹¶è¡Œå…¥é—¨

æˆ‘ä»¬å°†é‡‡ç”¨çš„å¹¶è¡ŒåŒ–æ–¹æ¡ˆç§°ä¸º**å¼ é‡å¹¶è¡Œï¼ˆTensor Parallelismï¼‰**ï¼Œæœ‰æ—¶ä¹Ÿè¢«ç§°ä¸º **NeMo-Megatron åˆ†ç‰‡**ã€‚

![å¼ é‡å¹¶è¡Œ](tensor-parallelism.png)

[Lightning AI çš„è¿™ä»½æ–‡æ¡£](https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/tp.html)å¯¹æ­¤æœ‰å‡ºè‰²çš„è§£é‡Šã€‚æ ¸å¿ƒæ€æƒ³æ˜¯æˆ‘ä»¬å¯ä»¥æ‰§è¡Œä¸¤ä¸ªçŸ©é˜µä¹˜æ³•ï¼ˆmatmulsï¼‰â€”â€”ä¸€ä¸ªæŒ‰åˆ—åˆ†ç‰‡ï¼Œå¦ä¸€ä¸ªæŒ‰è¡Œåˆ†ç‰‡â€”â€”**åªéœ€è¦ä¸€æ¬¡é›†åˆæ“ä½œï¼ˆall-reduceï¼‰**ã€‚

### å¼ é‡å¹¶è¡Œçš„å·¥ä½œåŸç†

åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå¤§å‹æ¨¡å‹é€šå¸¸ç”±å¤šä¸ªçº¿æ€§å±‚ç»„æˆã€‚å¼ é‡å¹¶è¡Œé€šè¿‡ä»¥ä¸‹æ–¹å¼å·¥ä½œï¼š

1. **æŒ‰åˆ—åˆ†ç‰‡çš„çŸ©é˜µä¹˜æ³•ï¼ˆColumn-wise shardingï¼‰**ï¼š
   - å°†æƒé‡çŸ©é˜µ W (shape: [d_in, d_out]) æŒ‰åˆ—åˆ†å‰²æˆ N ä»½
   - æ¯ä¸ªè®¾å¤‡æŒæœ‰ W_i (shape: [d_in, d_out/N])
   - è®¡ç®— Y_i = X @ W_iï¼ˆæ¯ä¸ªè®¾å¤‡ç‹¬ç«‹è®¡ç®—ï¼‰
   - æ— éœ€é€šä¿¡ï¼æ¯ä¸ªè®¾å¤‡çš„è¾“å‡ºç›´æ¥ç”¨äºä¸‹ä¸€å±‚

2. **æŒ‰è¡Œåˆ†ç‰‡çš„çŸ©é˜µä¹˜æ³•ï¼ˆRow-wise shardingï¼‰**ï¼š
   - å°†æƒé‡çŸ©é˜µ W (shape: [d_in, d_out]) æŒ‰è¡Œåˆ†å‰²
   - æ¯ä¸ªè®¾å¤‡æŒæœ‰ W_i (shape: [d_in/N, d_out])
   - è®¡ç®—éƒ¨åˆ†ç»“æœ Y_i = X_i @ W_i
   - éœ€è¦ All-Reduce æ“ä½œæ¥æ±‡æ€»æ‰€æœ‰è®¾å¤‡çš„ç»“æœ

3. **ç»„åˆç­–ç•¥**ï¼š
   - ç¬¬ä¸€ä¸ª matmulï¼šæŒ‰åˆ—åˆ†ç‰‡ï¼ˆæ— é€šä¿¡ï¼‰
   - ç¬¬äºŒä¸ª matmulï¼šæŒ‰è¡Œåˆ†ç‰‡ï¼ˆä¸€æ¬¡ all-reduceï¼‰
   - è¿™æ ·å¯ä»¥æœ€å°åŒ–é€šä¿¡å¼€é”€

### åˆ†ç‰‡æ–¹æ¡ˆ

å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®ä»¥ä¸‹æ–¹æ¡ˆå¯¹æƒé‡è¿›è¡Œåˆ†ç‰‡ï¼š

* **å¯¹äºæ³¨æ„åŠ›å—ï¼ˆAttention Blockï¼‰ï¼š**

  1. **Qã€K å’Œ V æŠ•å½±**æŒ‰**åˆ—**åˆ†ç‰‡ï¼ˆå› ä¸ºå®ƒä»¬ä»£è¡¨ç¬¬ä¸€ä¸ª matmulï¼‰
  2. **O æŠ•å½±**æŒ‰**è¡Œ**åˆ†ç‰‡ï¼ˆå› ä¸ºå®ƒæ˜¯ç¬¬äºŒä¸ª matmulï¼‰
  3. **æ³¨æ„åŠ›æœºåˆ¶**æœ¬èº«ä¸éœ€è¦é€šä¿¡ï¼Œå› ä¸ºå®ƒæ˜¯çº¯æ•°æ®å¹¶è¡Œçš„ï¼ˆå¤´çš„æ•°é‡è¢«åˆ†ç‰‡ï¼‰

* **å¯¹äºå‰é¦ˆç½‘ç»œï¼ˆFFNsï¼‰ï¼š**

  1. **Up å’Œ Gate æŠ•å½±**æŒ‰**åˆ—**åˆ†ç‰‡
  2. **Down æŠ•å½±**æŒ‰**è¡Œ**åˆ†ç‰‡

### ä¸ºä»€ä¹ˆè¿™æ ·åˆ†ç‰‡ï¼Ÿ

è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªå…·ä½“çš„ä¾‹å­æ¥ç†è§£ï¼š

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç®€å•çš„ä¸¤å±‚ç½‘ç»œï¼š
```
Input (batch, seq_len, hidden_dim)
  â†“
Layer 1: Linear (hidden_dim â†’ intermediate_dim)  # æŒ‰åˆ—åˆ†ç‰‡
  â†“
Activation
  â†“
Layer 2: Linear (intermediate_dim â†’ hidden_dim)  # æŒ‰è¡Œåˆ†ç‰‡
  â†“
Output (batch, seq_len, hidden_dim)
```

**æŒ‰åˆ—åˆ†ç‰‡ Layer 1**ï¼š
- è®¾å¤‡ 0 è®¡ç®—å‰åŠéƒ¨åˆ†çš„ intermediate_dim
- è®¾å¤‡ 1 è®¡ç®—ååŠéƒ¨åˆ†çš„ intermediate_dim
- æ— éœ€é€šä¿¡ï¼

**æŒ‰è¡Œåˆ†ç‰‡ Layer 2**ï¼š
- è®¾å¤‡ 0 ä½¿ç”¨å‰åŠéƒ¨åˆ†çš„ä¸­é—´ç»“æœ
- è®¾å¤‡ 1 ä½¿ç”¨ååŠéƒ¨åˆ†çš„ä¸­é—´ç»“æœ
- éœ€è¦ All-Reduce æ¥ç»„åˆæœ€ç»ˆç»“æœ

-----

## JAX å¹¶è¡Œæ”¯æŒå…¥é—¨

### gSPMD æ¨¡å¼

ä¸ PyTorch ä¸åŒï¼ŒJAX çš„å¹¶è¡Œæ”¯æŒä½¿ç”¨ **gSPMDï¼ˆé€šç”¨å•ç¨‹åºå¤šæ•°æ®ï¼‰æ¨¡å¼**ã€‚è¿™æ„å‘³ç€ï¼š

**PyTorch æ–¹å¼ï¼ˆä¼ ç»Ÿ SPMDï¼‰**ï¼š
- æ¯ä¸ªè®¾å¤‡ä¸€ä¸ªè¿›ç¨‹
- æ‰‹åŠ¨ç®¡ç†é›†åˆæ“ä½œï¼ˆall-reduceã€all-gather ç­‰ï¼‰
- æ˜¾å¼æ•°æ®åˆ†å‘å’ŒåŒæ­¥

**JAX æ–¹å¼ï¼ˆgSPMDï¼‰**ï¼š
- å•ä¸ªç¨‹åºæ§åˆ¶æ‰€æœ‰è®¾å¤‡
- åªéœ€æŒ‡å®š `mesh`ï¼ˆè®¾å¤‡ç½‘æ ¼ï¼‰å’Œåˆ†ç‰‡æ–¹å¼
- XLA ç¼–è¯‘å™¨è‡ªåŠ¨æ’å…¥å¿…è¦çš„é›†åˆæ“ä½œ

### gSPMD çš„ä¼˜åŠ¿

1. **ç®€åŒ–ç¼–ç¨‹æ¨¡å‹**ï¼šæ— éœ€æ‰‹åŠ¨ç®¡ç†è¿›ç¨‹é—´é€šä¿¡
2. **è‡ªåŠ¨ä¼˜åŒ–**ï¼šç¼–è¯‘å™¨å¯ä»¥ä¼˜åŒ–é€šä¿¡æ¨¡å¼
3. **æ›´å°‘çš„é”™è¯¯**ï¼šå‡å°‘äº†æ‰‹åŠ¨ç®¡ç†å¸¦æ¥çš„åŒæ­¥é—®é¢˜

è¯¦ç»†è¿‡ç¨‹åœ¨è¿™é‡Œæœ‰éå¸¸è¯¦ç»†çš„æè¿°ï¼š[JAX åˆ†å¸ƒå¼æ•°ç»„å’Œè‡ªåŠ¨å¹¶è¡ŒåŒ–](https://docs.jax.dev/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html)ã€‚

### å®ç°å¹¶è¡Œçš„ä¸¤ä¸ªå…³é”®è¦ç´ 

æœ¬è´¨ä¸Šï¼Œè¦å¹¶è¡Œè¿è¡Œæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸¤ä¸ªå…³é”®è¦ç´ ï¼š

1. **å®šä¹‰ä¸€ä¸ª `mesh`ï¼ˆç½‘æ ¼ï¼‰**ï¼šåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå®ƒåªæ˜¯ `jax.make_mesh((jax.device_count(), ), ('axis', ))`ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬ç»™è½´çš„åç§°å¯¹åŠŸèƒ½å½±å“ä¸å¤§ï¼Œä½†æœ‰åŠ©äºä»£ç å¯è¯»æ€§ã€‚

2. **çŸ¥é“æ¨¡å‹çš„æ¯ä¸ªæƒé‡æ˜¯å¦‚ä½•åˆ†ç‰‡çš„**ã€‚

### Mesh çš„æ¦‚å¿µ

Mesh æ˜¯ JAX ä¸­ç»„ç»‡è®¾å¤‡çš„æ–¹å¼ã€‚å¯ä»¥æƒ³è±¡æˆä¸€ä¸ªå¤šç»´ç½‘æ ¼ï¼š

```python
# 1D mesh: 8 ä¸ªè®¾å¤‡æ’æˆä¸€è¡Œ
mesh_1d = jax.make_mesh((8,), ('data',))

# 2D mesh: 8 ä¸ªè®¾å¤‡æ’æˆ 2x4 çš„ç½‘æ ¼
mesh_2d = jax.make_mesh((2, 4), ('data', 'model'))

# åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œä½¿ç”¨ 1D mesh
mesh = jax.make_mesh((jax.device_count(),), ('axis',))
```

ä¸ºäº†å¼„æ¸…æ¥šç¬¬äºŒç‚¹ï¼Œè®©æˆ‘ä»¬æ‰“å°å‡ºæ¨¡å‹çš„æƒé‡å¹¶å†³å®šå®ƒä»¬çš„åˆ†ç‰‡æ–¹å¼ã€‚

-----

## æƒé‡åˆ†ç‰‡

### æ£€æŸ¥æ¨¡å‹æƒé‡

è®©æˆ‘ä»¬æ‰“å°æƒé‡ä»¥äº†è§£æˆ‘ä»¬æ­£åœ¨å¤„ç†ä»€ä¹ˆã€‚åœ¨ `weights, func = torchax.extract_jax(model)` ä¹‹åæ·»åŠ ä»¥ä¸‹ä»£ç ï¼š

```python
for name, w in weights.items():
  print(name, w.shape)
```

æˆ‘ä»¬å°†å¾—åˆ°ç±»ä¼¼è¿™æ ·çš„è¾“å‡ºï¼š

```
model.rotary_emb.inv_freq (64,)
model.embed_tokens.weight (32000, 4096)
model.layers.0.self_attn.q_proj.weight (4096, 4096)
model.layers.0.self_attn.k_proj.weight (4096, 4096)
model.layers.0.self_attn.v_proj.weight (4096, 4096)
model.layers.0.self_attn.o_proj.weight (4096, 4096)
model.layers.0.mlp.gate_proj.weight (11008, 4096)
model.layers.0.mlp.up_proj.weight (11008, 4096)
model.layers.0.mlp.down_proj.weight (4096, 11008)
model.layers.0.input_layernorm.weight (4096,)
model.layers.0.post_attention_layernorm.weight (4096,)
model.layers.1.self_attn.q_proj.weight (4096, 4096)
...
```

### ç†è§£æƒé‡ç»´åº¦

- **Embedding å±‚**: `(32000, 4096)` - è¯æ±‡è¡¨å¤§å° Ã— éšè—ç»´åº¦
- **Q/K/V æŠ•å½±**: `(4096, 4096)` - éšè—ç»´åº¦ Ã— éšè—ç»´åº¦
- **O æŠ•å½±**: `(4096, 4096)` - éšè—ç»´åº¦ Ã— éšè—ç»´åº¦
- **FFN Gate/Up**: `(11008, 4096)` - ä¸­é—´ç»´åº¦ Ã— éšè—ç»´åº¦
- **FFN Down**: `(4096, 11008)` - éšè—ç»´åº¦ Ã— ä¸­é—´ç»´åº¦

### åˆ†ç‰‡æ ‡æ³¨

æƒé‡è·¨è¶Š 32 å±‚ã€‚æ ¹æ®æˆ‘ä»¬ä¹‹å‰çš„è®¨è®ºï¼Œæˆ‘ä»¬éœ€è¦æŒ‰å¦‚ä¸‹æ–¹å¼å¯¹å®ƒä»¬è¿›è¡Œåˆ†ç‰‡ï¼š

```
model.layers.0.self_attn.q_proj.weight (4096, 4096) -> ('axis', None)
model.layers.0.self_attn.k_proj.weight (4096, 4096) -> ('axis', None)
model.layers.0.self_attn.v_proj.weight (4096, 4096) -> ('axis', None)
model.layers.0.self_attn.o_proj.weight (4096, 4096) -> (None, 'axis')
model.layers.0.mlp.gate_proj.weight (11008, 4096) -> ('axis', None)
model.layers.0.mlp.up_proj.weight (11008, 4096) -> ('axis', None)
model.layers.0.mlp.down_proj.weight (4096, 11008) -> (None, 'axis')
```

**åˆ†ç‰‡è¯´æ˜**ï¼š
- `('axis', None)` è¡¨ç¤ºæ²¿ç¬¬ä¸€ä¸ªç»´åº¦ï¼ˆè¡Œï¼‰åˆ†ç‰‡ï¼Œç¬¬äºŒä¸ªç»´åº¦å¤åˆ¶
- `(None, 'axis')` è¡¨ç¤ºç¬¬ä¸€ä¸ªç»´åº¦å¤åˆ¶ï¼Œæ²¿ç¬¬äºŒä¸ªç»´åº¦ï¼ˆåˆ—ï¼‰åˆ†ç‰‡
- `P()` è¡¨ç¤ºå®Œå…¨å¤åˆ¶ï¼ˆä¸åˆ†ç‰‡ï¼‰

### ä¸ºä»€ä¹ˆè¿™æ ·æ ‡æ³¨ï¼Ÿ

ä»¥ q_proj ä¸ºä¾‹ï¼Œæƒé‡å½¢çŠ¶æ˜¯ `(4096, 4096)`ï¼š
- æ ‡æ³¨ä¸º `('axis', None)` æ„å‘³ç€ï¼š
  - åœ¨ 8 ä¸ªè®¾å¤‡ä¸Šï¼Œæ¯ä¸ªè®¾å¤‡æŒæœ‰ `(512, 4096)` çš„æƒé‡
  - è¾“å…¥ `(batch, seq_len, 4096)` è¢«å¤åˆ¶åˆ°æ‰€æœ‰è®¾å¤‡
  - æ¯ä¸ªè®¾å¤‡è®¡ç®— `(batch, seq_len, 512)` çš„è¾“å‡º
  - æ‹¼æ¥åå¾—åˆ°å®Œæ•´çš„ `(batch, seq_len, 4096)` è¾“å‡º

### å®ç°åˆ†ç‰‡å‡½æ•°

é™¤äº†è®¨è®ºçš„æƒé‡å¤–ï¼Œè¿˜æœ‰åµŒå…¥æƒé‡å’Œæœ€ç»ˆè¾“å‡ºæŠ•å½±çš„æƒé‡ã€‚å¯¹äºè¿™äº›ï¼Œæˆ‘ä»¬åœ¨åˆ†ç‰‡ä¸Šæœ‰æ›´å¤§çš„çµæ´»æ€§ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·ç¼–å†™åˆ†ç‰‡å‡½æ•°ï¼š

```python
from jax.sharding import NamedSharding, PartitionSpec as P

def shard_weights_llama(mesh, weights):
  """
  ä¸º Llama æ¨¡å‹çš„æƒé‡åº”ç”¨å¼ é‡å¹¶è¡Œåˆ†ç‰‡
  
  å‚æ•°:
    mesh: JAX è®¾å¤‡ç½‘æ ¼
    weights: æ¨¡å‹æƒé‡å­—å…¸
  
  è¿”å›:
    åˆ†ç‰‡åçš„æƒé‡å­—å…¸
  """
  result = {}
  for k, v in weights.items():
    # Qã€Kã€Vã€Gateã€Up æŠ•å½±ï¼šæŒ‰åˆ—åˆ†ç‰‡
    if (('q_proj' in k) or
        ('k_proj' in k) or
        ('v_proj' in k) or
        ('gate_proj' in k) or
        ('up_proj' in k)):
      sharding = P('axis', None)
    # Oã€Downã€LM Headã€Embeddingï¼šæŒ‰è¡Œåˆ†ç‰‡
    elif(('o_proj' in k) or
        ('down_proj' in k) or
        ('lm_head.weight' in k) or
        ('embed_tokens' in k)):
      sharding = P(None, 'axis')
    # å…¶ä»–æƒé‡ï¼ˆLayerNorm ç­‰ï¼‰ï¼šå®Œå…¨å¤åˆ¶
    else:
      sharding = P() # replicated
    result[k] = jax.device_put(v, NamedSharding(mesh, sharding))
  return result
```

### åˆ†ç‰‡å‡½æ•°è¯¦è§£

è®©æˆ‘ä»¬æ·±å…¥ç†è§£è¿™ä¸ªå‡½æ•°ï¼š

```python
# åˆ›å»ºåˆ†ç‰‡è§„èŒƒ
sharding = P('axis', None)  # ç¬¬ä¸€ç»´æ²¿ 'axis' åˆ†ç‰‡ï¼Œç¬¬äºŒç»´å¤åˆ¶

# å°†æ•°ç»„æ”¾ç½®åˆ°è®¾å¤‡ä¸Šå¹¶åº”ç”¨åˆ†ç‰‡
result[k] = jax.device_put(v, NamedSharding(mesh, sharding))
```

**`jax.device_put` çš„ä½œç”¨**ï¼š
1. å°†æ•°ç»„åˆ†å‘åˆ°å¤šä¸ªè®¾å¤‡
2. æ ¹æ® sharding è§„èŒƒåˆ‡åˆ†æ•°æ®
3. è¿”å›ä¸€ä¸ªé€»è¾‘ä¸Šç»Ÿä¸€ä½†ç‰©ç†ä¸Šåˆ†å¸ƒçš„æ•°ç»„

**NamedSharding çš„ä¼˜åŠ¿**ï¼š
- ä½¿ç”¨å‘½åè½´ï¼ˆå¦‚ 'axis'ï¼‰è€Œä¸æ˜¯æ•°å­—ç´¢å¼•
- æ›´æ˜“è¯»å’Œç»´æŠ¤
- å¯ä»¥è½»æ¾é‡æ–°é…ç½®åˆ†ç‰‡æ–¹æ¡ˆ

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `weights = shard_weights_llama(mesh, weights)` å¯¹æƒé‡è¿›è¡Œåˆ†ç‰‡ã€‚

-----

## å†æ¬¡è¿è¡Œ

### å‡†å¤‡è¾“å…¥æ•°æ®

ç°åœ¨æƒé‡å·²ç»åˆ†ç‰‡ï¼Œæˆ‘ä»¬å‡ ä¹å‡†å¤‡å¥½ä»¥åˆ†å¸ƒå¼æ–¹å¼è¿è¡Œæ¨ç†äº†ï¼è¿˜æœ‰ä¸€æ­¥ï¼šè¾“å…¥ä¹Ÿéœ€è¦åœ¨æ¯ä¸ªè®¾å¤‡ä¸Šå¯ç”¨ï¼Œä»¥ä¾¿æ‰€æœ‰è®¾å¤‡éƒ½å¯ä»¥ä½¿ç”¨å®ƒè¿›è¡Œè®¡ç®—ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å¤åˆ¶è¾“å…¥æ¥å®ç°è¿™ä¸€ç‚¹ï¼š

```python
model_inputs.input_ids = jax.device_put(
  model_inputs.input_ids, NamedSharding(mesh, P())) # replicate
```

**ä¸ºä»€ä¹ˆå¤åˆ¶è¾“å…¥ï¼Ÿ**

- è¾“å…¥æ•°æ® `(batch, seq_len)` éœ€è¦åœ¨æ‰€æœ‰è®¾å¤‡ä¸Šå¯ç”¨
- æ¯ä¸ªè®¾å¤‡ä½¿ç”¨ç›¸åŒçš„è¾“å…¥ï¼Œä½†ä½¿ç”¨ä¸åŒçš„æƒé‡åˆ†ç‰‡
- `P()` è¡¨ç¤ºå®Œå…¨å¤åˆ¶ï¼ˆæ‰€æœ‰è®¾å¤‡æŒæœ‰ç›¸åŒçš„å‰¯æœ¬ï¼‰

### å®Œæ•´çš„åˆ†å¸ƒå¼æ¨ç†ä»£ç 

```python
import jax
import torchax
from transformers import AutoModelForCausalLM, AutoTokenizer
from jax.sharding import NamedSharding, PartitionSpec as P
import time

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf", 
    torch_dtype="bfloat16", 
    device_map="cpu"
)
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

# è½¬æ¢ä¸º JAX
weights, func = torchax.extract_jax(model)

# åˆ›å»ºè®¾å¤‡ç½‘æ ¼
mesh = jax.make_mesh((jax.device_count(),), ('axis',))

# åˆ†ç‰‡æƒé‡
weights = shard_weights_llama(mesh, weights)

# å‡†å¤‡è¾“å…¥
model_inputs = tokenizer(["The secret to baking a good cake is "], return_tensors="jax")
model_inputs.input_ids = jax.device_put(
    model_inputs.input_ids, NamedSharding(mesh, P()))

# åˆ›å»º JIT ç¼–è¯‘çš„å‡½æ•°
def forward_pass(weights, input_ids):
    return func(weights, (input_ids,), {'use_cache': False})

jitted_func = jax.jit(forward_pass)

# æ€§èƒ½æµ‹è¯•
for i in range(3):
    start = time.time()
    res = jitted_func(weights, model_inputs.input_ids)
    jax.block_until_ready(res)
    end = time.time()
    print(f"{i} {end - start} seconds")
```

### æ€§èƒ½ç»“æœ

å†æ¬¡è¿è¡Œè„šæœ¬ä¼šäº§ç”Ÿï¼š

```
0 5.062012195587158 seconds
1 0.0038039684295654297 seconds
2 0.0034346580505371094 seconds
```

è¿™æ¯”å•è®¾å¤‡ç‰ˆæœ¬**å¿«çº¦ 4.3 å€**ã€‚ğŸš€

### æ€§èƒ½åˆ†æ

**ä¸ºä»€ä¹ˆä¸æ˜¯ 8 å€åŠ é€Ÿï¼Ÿ**

ç†æƒ³æƒ…å†µä¸‹ï¼Œä½¿ç”¨ 8 ä¸ªè®¾å¤‡åº”è¯¥è·å¾— 8 å€åŠ é€Ÿï¼Œä½†å®é™…ä¸Šåªæœ‰ 4.3 å€ã€‚åŸå› åŒ…æ‹¬ï¼š

1. **é€šä¿¡å¼€é”€**ï¼šAll-reduce æ“ä½œéœ€è¦æ—¶é—´
2. **è´Ÿè½½ä¸å¹³è¡¡**ï¼šæŸäº›æ“ä½œå¯èƒ½æ— æ³•å®Œç¾å¹¶è¡Œ
3. **å†…å­˜å¸¦å®½é™åˆ¶**ï¼šè®¾å¤‡é—´æ•°æ®ä¼ è¾“çš„ç“¶é¢ˆ
4. **æœªåˆ†ç‰‡çš„æ“ä½œ**ï¼šLayerNorm ç­‰æ“ä½œæ˜¯å¤åˆ¶çš„

**å•è®¾å¤‡æ€§èƒ½å¯¹æ¯”**ï¼ˆä»ç¬¬ä¸€éƒ¨åˆ†ï¼‰ï¼š
- ç¬¬ä¸€æ¬¡è¿è¡Œï¼ˆå«ç¼–è¯‘ï¼‰ï¼š4.37 ç§’
- åç»­è¿è¡Œï¼š0.013 ç§’

**8 è®¾å¤‡æ€§èƒ½**ï¼š
- ç¬¬ä¸€æ¬¡è¿è¡Œï¼ˆå«ç¼–è¯‘ï¼‰ï¼š5.06 ç§’
- åç»­è¿è¡Œï¼š0.0038 ç§’ï¼ˆ**3.4 å€å¿«äºå•è®¾å¤‡**ï¼‰

-----

## å¦‚ä½•ç¡®ä¿å®ƒçœŸçš„åœ¨ 8 ä¸ªè®¾å¤‡ä¸Šè¿è¡Œï¼Ÿ

### ä½¿ç”¨ JAX Profiler

è™½ç„¶æˆ‘ä»¬çœ‹åˆ°äº†æ¨ç†é€Ÿåº¦çš„æå‡ï¼Œä½†å®ƒä¸æ˜¯å®Œæ•´çš„ 8 å€åŠ é€Ÿã€‚ä¸ºäº†ç¡®è®¤å®ƒç¡®å®åˆ©ç”¨äº†æ‰€æœ‰ 8 ä¸ªè®¾å¤‡å¹¶ç†è§£ä¸ºä»€ä¹ˆåŠ é€Ÿä¸æ˜¯çº¿æ€§çš„ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ [JAX profiler](https://docs.jax.dev/en/latest/profiling.html)ã€‚

è¦æ•è·æ€§èƒ½åˆ†æï¼Œåªéœ€ä½¿ç”¨æ ‡å‡†çš„ JAX API åŒ…è£…ç›¸å…³ä»£ç éƒ¨åˆ†ï¼š

```python
with jax.profiler.trace("/tmp/jax-trace", create_perfetto_link=False):
  # æ‚¨çš„æ¨ç†ä»£ç åœ¨è¿™é‡Œ
  for i in range(3):
    start = time.time()
    res = jitted_func(weights, model_inputs.input_ids)
    jax.block_until_ready(res)
    end = time.time()
    print(f"{i} {end - start} seconds")
```

### åˆ†æå·¥å…·é€‰æ‹©

**Perfetto**ï¼ˆæ¨èç”¨äºæœ¬åœ°æœºå™¨ï¼‰ï¼š
```python
with jax.profiler.trace("/tmp/jax-trace", create_perfetto_link=True):
    # ä»£ç 
```
- ä¼šç”Ÿæˆä¸€ä¸ª Perfetto é“¾æ¥
- åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€å¯è§†åŒ–
- äº¤äº’å¼æ—¶é—´çº¿è§†å›¾

**TensorBoard**ï¼ˆé€‚åˆè¿œç¨‹æœåŠ¡å™¨ï¼‰ï¼š
```python
with jax.profiler.trace("/tmp/jax-trace", create_perfetto_link=False):
    # ä»£ç 

# ç„¶åè¿è¡Œ
# tensorboard --logdir=/tmp/jax-trace
```

### æ€§èƒ½åˆ†æç¤ºä¾‹

ç”±äºåœ¨è¿œç¨‹æœºå™¨ä¸Šï¼Œæˆ‘ä½¿ç”¨äº† TensorBoard çš„ xprof æ’ä»¶è€Œä¸æ˜¯ Perfettoã€‚æ— è®ºå¦‚ä½•ï¼Œç»“æœéƒ½æ˜¯è¿™æ ·çš„è§†è§‰è¡¨ç¤ºï¼š

![æ€§èƒ½åˆ†æ](image.png)

### å¦‚ä½•è§£è¯»æ€§èƒ½åˆ†æå›¾

ä»è¿™ä¸ªè¾“å‡ºä¸­ï¼Œæ‚¨å¯ä»¥ï¼š

1. **éªŒè¯è®¾å¤‡æ´»åŠ¨**ï¼š
   - ç¡®è®¤æ‰€æœ‰ 8 ä¸ªè®¾å¤‡éƒ½å¤„äºæ´»åŠ¨çŠ¶æ€
   - æŸ¥çœ‹æ¯ä¸ªè®¾å¤‡çš„åˆ©ç”¨ç‡

2. **è¯†åˆ«æ“ä½œ**ï¼š
   - æŸ¥çœ‹å“ªäº›æ“ä½œåœ¨æ¯ä¸ªè®¾å¤‡ä¸Šè¿è¡Œ
   - äº†è§£æ“ä½œçš„æ‰§è¡Œé¡ºåº

3. **å‘ç°ç“¶é¢ˆ**ï¼š
   - è¯†åˆ«é€šä¿¡å¼€é”€ï¼ˆAll-reduce æ“ä½œï¼‰
   - å‘ç°è´Ÿè½½ä¸å¹³è¡¡
   - æ‰¾å‡ºç©ºé—²æ—¶é—´

4. **ä¼˜åŒ–æœºä¼š**ï¼š
   - çœ‹å“ªäº›æ“ä½œå ç”¨æœ€å¤šæ—¶é—´
   - ç¡®å®šå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–çš„éƒ¨åˆ†

### æ€§èƒ½åˆ†æçš„å…³é”®æŒ‡æ ‡

**æŸ¥æ‰¾å†…å®¹**ï¼š
- **è®¾å¤‡åˆ©ç”¨ç‡**ï¼šæ‰€æœ‰è®¾å¤‡éƒ½åœ¨å·¥ä½œå—ï¼Ÿ
- **é€šä¿¡æ—¶é—´**ï¼šAll-reduce å ç”¨å¤šå°‘æ—¶é—´ï¼Ÿ
- **è®¡ç®—æ—¶é—´**ï¼šå®é™…è®¡ç®—å ç”¨å¤šå°‘æ—¶é—´ï¼Ÿ
- **ç©ºé—²æ—¶é—´**ï¼šè®¾å¤‡ç­‰å¾…çš„æ—¶é—´

**ä¼˜åŒ–ç›®æ ‡**ï¼š
- æœ€å¤§åŒ–è®¾å¤‡åˆ©ç”¨ç‡
- æœ€å°åŒ–é€šä¿¡å¼€é”€
- å¹³è¡¡å„è®¾å¤‡çš„è´Ÿè½½

### é‡ç°ç»“æœ

è¦é‡ç°æœ¬æ–‡çš„å†…å®¹ï¼Œè¯·è¿è¡Œï¼š
```python
python jax_hg_02.py
```

-----

## é«˜çº§è¯é¢˜ï¼šæ›´å¤æ‚çš„åˆ†ç‰‡æ–¹æ¡ˆ

### 2D å¹¶è¡Œï¼šæ•°æ®å¹¶è¡Œ + å¼ é‡å¹¶è¡Œ

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ‚¨å¯èƒ½æƒ³è¦ç»“åˆå¤šç§å¹¶è¡Œç­–ç•¥ï¼š

```python
# åˆ›å»º 2D mesh: 2 ä¸ªæ•°æ®å¹¶è¡Œ Ã— 4 ä¸ªæ¨¡å‹å¹¶è¡Œ
mesh = jax.make_mesh((2, 4), ('data', 'model'))

# æƒé‡åˆ†ç‰‡
def shard_weights_2d(mesh, weights):
    result = {}
    for k, v in weights.items():
        if 'q_proj' in k:
            # åªåœ¨æ¨¡å‹ç»´åº¦åˆ†ç‰‡ï¼Œæ•°æ®ç»´åº¦å¤åˆ¶
            sharding = P(None, 'model')
        elif 'o_proj' in k:
            # åœ¨æ¨¡å‹ç»´åº¦åˆ†ç‰‡
            sharding = P('model', None)
        # ... å…¶ä»–æƒé‡
        result[k] = jax.device_put(v, NamedSharding(mesh, sharding))
    return result

# è¾“å…¥åˆ†ç‰‡
# åœ¨æ•°æ®ç»´åº¦åˆ†ç‰‡ï¼ˆbatchï¼‰
input_sharding = P('data', None)
model_inputs.input_ids = jax.device_put(
    model_inputs.input_ids, 
    NamedSharding(mesh, input_sharding)
)
```

### æµæ°´çº¿å¹¶è¡Œ

å¯¹äºéå¸¸å¤§çš„æ¨¡å‹ï¼Œæ‚¨è¿˜å¯ä»¥æ·»åŠ æµæ°´çº¿å¹¶è¡Œï¼š

```python
# 3D mesh: æ•°æ®å¹¶è¡Œ Ã— å¼ é‡å¹¶è¡Œ Ã— æµæ°´çº¿å¹¶è¡Œ
mesh = jax.make_mesh((2, 2, 2), ('data', 'model', 'pipeline'))

# å°†ä¸åŒçš„å±‚æ”¾åœ¨ä¸åŒçš„æµæ°´çº¿é˜¶æ®µ
def shard_with_pipeline(mesh, weights):
    result = {}
    for k, v in weights.items():
        # æ ¹æ®å±‚å·ç¡®å®šæµæ°´çº¿é˜¶æ®µ
        layer_num = extract_layer_number(k)
        pipeline_stage = layer_num // 16  # å‡è®¾ 32 å±‚åˆ†æˆ 2 ä¸ªé˜¶æ®µ
        
        # ... åº”ç”¨åˆ†ç‰‡
        result[k] = jax.device_put(v, sharding)
    return result
```

-----

## æœ€ä½³å®è·µå’ŒæŠ€å·§

### 1. é€‰æ‹©åˆé€‚çš„åˆ†ç‰‡ç­–ç•¥

```python
# å¯¹äºå°æ¨¡å‹ï¼ˆ< 1B å‚æ•°ï¼‰
# ä½¿ç”¨ç®€å•çš„æ•°æ®å¹¶è¡Œ
mesh = jax.make_mesh((jax.device_count(),), ('data',))

# å¯¹äºä¸­å‹æ¨¡å‹ï¼ˆ1B - 10B å‚æ•°ï¼‰
# ä½¿ç”¨å¼ é‡å¹¶è¡Œ
mesh = jax.make_mesh((jax.device_count(),), ('model',))

# å¯¹äºå¤§å‹æ¨¡å‹ï¼ˆ> 10B å‚æ•°ï¼‰
# ä½¿ç”¨ 2D æˆ– 3D å¹¶è¡Œ
mesh = jax.make_mesh((2, 4), ('data', 'model'))
```

### 2. éªŒè¯åˆ†ç‰‡æ­£ç¡®æ€§

```python
# æ£€æŸ¥æƒé‡åˆ†ç‰‡
for k, v in weights.items():
    print(f"{k}: {v.sharding}")

# æ£€æŸ¥æ˜¯å¦çœŸçš„åˆ†ç‰‡äº†
assert v.sharding.is_fully_sharded or v.sharding.is_fully_replicated
```

### 3. ç›‘æ§å†…å­˜ä½¿ç”¨

```python
# æ‰“å°æ¯ä¸ªè®¾å¤‡çš„å†…å­˜ä½¿ç”¨
for device in jax.devices():
    mem_info = device.memory_stats()
    print(f"Device {device.id}: {mem_info['bytes_in_use'] / 1e9:.2f} GB")
```

### 4. è°ƒè¯•åˆ†ç‰‡é—®é¢˜

```python
# ä½¿ç”¨ jax.debug æŸ¥çœ‹åˆ†ç‰‡
import jax.debug as jdb

def debug_forward(weights, input_ids):
    jdb.print("Input sharding: {}", input_ids.sharding)
    result = func(weights, (input_ids,), {'use_cache': False})
    jdb.print("Output sharding: {}", result.logits.sharding)
    return result
```

-----

## å¸¸è§é—®é¢˜è§£ç­”

### Q: ä¸ºä»€ä¹ˆæˆ‘çš„åŠ é€Ÿæ¯”ä¸æ˜¯çº¿æ€§çš„ï¼Ÿ

A: éçº¿æ€§åŠ é€Ÿçš„å¸¸è§åŸå› ï¼š
1. **é€šä¿¡å¼€é”€**ï¼šè®¾å¤‡é—´æ•°æ®ä¼ è¾“éœ€è¦æ—¶é—´
2. **Amdahl å®šå¾‹**ï¼šéƒ¨åˆ†ä»£ç æ— æ³•å¹¶è¡ŒåŒ–
3. **è´Ÿè½½ä¸å¹³è¡¡**ï¼šæŸäº›è®¾å¤‡å¯èƒ½æ¯”å…¶ä»–è®¾å¤‡å·¥ä½œæ›´å¤š
4. **å†…å­˜å¸¦å®½**ï¼šå¯èƒ½å—é™äºå†…å­˜è®¿é—®é€Ÿåº¦

### Q: å¦‚ä½•é€‰æ‹©åˆ†ç‰‡ç»´åº¦ï¼Ÿ

A: ä¸€èˆ¬åŸåˆ™ï¼š
- **æƒé‡çŸ©é˜µ**ï¼šæ ¹æ®è®¡ç®—å›¾é€‰æ‹©ï¼ˆåˆ—åˆ†ç‰‡ vs è¡Œåˆ†ç‰‡ï¼‰
- **æ¿€æ´»å€¼**ï¼šé€šå¸¸åœ¨ batch ç»´åº¦åˆ†ç‰‡ï¼ˆæ•°æ®å¹¶è¡Œï¼‰
- **æ¢¯åº¦**ï¼šåº”è¯¥ä¸æƒé‡çš„åˆ†ç‰‡æ–¹å¼åŒ¹é…

### Q: æˆ‘å¯ä»¥åœ¨å•ä¸ª GPU ä¸Šæµ‹è¯•åˆ†ç‰‡ä»£ç å—ï¼Ÿ

A: å¯ä»¥ï¼JAX å¯ä»¥æ¨¡æ‹Ÿå¤šä¸ªè®¾å¤‡ï¼š
```python
import os
os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=8'
# ç°åœ¨ jax.device_count() ä¼šè¿”å› 8ï¼Œå³ä½¿æ‚¨åªæœ‰ 1 ä¸ª GPU
```

### Q: å¦‚ä½•å¤„ç†åŠ¨æ€å½¢çŠ¶ï¼Ÿ

A: å¯¹äºåŠ¨æ€å½¢çŠ¶ï¼ˆå¦‚å¯å˜åºåˆ—é•¿åº¦ï¼‰ï¼Œè€ƒè™‘ï¼š
1. **å¡«å……åˆ°å›ºå®šé•¿åº¦**ï¼šæœ€ç®€å•ä½†å¯èƒ½æµªè´¹è®¡ç®—
2. **Bucketing**ï¼šå°†ç›¸ä¼¼é•¿åº¦çš„åºåˆ—åˆ†ç»„
3. **åŠ¨æ€åˆ†ç‰‡**ï¼šåœ¨æ¯æ¬¡è¿­ä»£æ—¶é‡æ–°åˆ†ç‰‡ï¼ˆè¾ƒæ…¢ï¼‰

### Q: åˆ†ç‰‡æ˜¯å¦å½±å“æ•°å€¼ç²¾åº¦ï¼Ÿ

A: ç†è®ºä¸Šä¸åº”è¯¥ï¼Œä½†è¦æ³¨æ„ï¼š
1. **All-reduce é¡ºåº**ï¼šæµ®ç‚¹è¿ç®—ä¸å®Œå…¨æ»¡è¶³ç»“åˆå¾‹
2. **èˆå…¥è¯¯å·®**ï¼šåˆ†å¸ƒå¼è®¡ç®—å¯èƒ½æœ‰è½»å¾®çš„æ•°å€¼å·®å¼‚
3. **å»ºè®®**ï¼šä½¿ç”¨ç›¸åŒçš„ç§å­å¹¶éªŒè¯è¾“å‡º

-----

## ç»“è®º

æˆ‘ä»¬æˆåŠŸå±•ç¤ºäº†å¦‚ä½•åœ¨**ä¸ä¿®æ”¹æ¨¡å‹æ ¸å¿ƒä»£ç **çš„æƒ…å†µä¸‹ï¼Œä»¥**åˆ†å¸ƒå¼æ–¹å¼**è¿è¡Œ Llama æ¨¡å‹çš„å‰å‘ä¼ æ’­ã€‚å…³é”®åœ¨äºç®€å•åœ°æŒ‡å®šæƒé‡åº”è¯¥å¦‚ä½•åˆ†ç‰‡ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨æ ‡å‡†çš„ JAX æ€§èƒ½åˆ†æå·¥å…·æ¥ç¡®è®¤åˆ†å¸ƒå¼æ‰§è¡Œå¹¶å¸®åŠ©è¿›è¡Œæ€§èƒ½åˆ†æã€‚

### å…³é”®è¦ç‚¹

âœ… **å¼ é‡å¹¶è¡Œ**ï¼šé€šè¿‡åˆ†ç‰‡æƒé‡å®ç°æ¨¡å‹å¹¶è¡Œ
âœ… **gSPMD**ï¼šJAX çš„è‡ªåŠ¨å¹¶è¡ŒåŒ–ç®€åŒ–äº†ç¼–ç¨‹
âœ… **æ€§èƒ½æå‡**ï¼šåœ¨ 8 ä¸ªè®¾å¤‡ä¸Šè·å¾—çº¦ 4.3 å€åŠ é€Ÿ
âœ… **æ˜“äºå®ç°**ï¼šåªéœ€å®šä¹‰ mesh å’Œåˆ†ç‰‡è§„èŒƒ
âœ… **æ€§èƒ½åˆ†æ**ï¼šä½¿ç”¨ JAX profiler éªŒè¯å’Œä¼˜åŒ–

### æœªæ¥æ–¹å‘

åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†å¯¹ HuggingFace diffusers åº“ä¸­çš„æ¨¡å‹æ‰§è¡Œç›¸åŒçš„æ“ä½œï¼Œå±•ç¤ºè¿™ç§æ–¹æ³•çš„é€šç”¨æ€§ã€‚

-----

## æ‰©å±•é˜…è¯»

### JAX å¹¶è¡ŒåŒ–èµ„æº

1. **å®˜æ–¹æ–‡æ¡£**ï¼š
   - [JAX åˆ†å¸ƒå¼æ•°ç»„](https://docs.jax.dev/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html)
   - [JAX sharding æ•™ç¨‹](https://docs.jax.dev/en/latest/sharded-computation.html)

2. **æ€§èƒ½ä¼˜åŒ–**ï¼š
   - [JAX æ€§èƒ½åˆ†æ](https://docs.jax.dev/en/latest/profiling.html)
   - [XLA ä¼˜åŒ–æŒ‡å—](https://www.tensorflow.org/xla/performance)

3. **å¹¶è¡Œç­–ç•¥**ï¼š
   - [Megatron-LM è®ºæ–‡](https://arxiv.org/abs/1909.08053)
   - [GPipe è®ºæ–‡](https://arxiv.org/abs/1811.06965)

### å®è·µé¡¹ç›®

1. **å®éªŒä¸åŒçš„åˆ†ç‰‡ç­–ç•¥**ï¼š
   - å°è¯• 2D å¹¶è¡Œï¼ˆæ•°æ® + æ¨¡å‹ï¼‰
   - æ¯”è¾ƒä¸åŒé…ç½®çš„æ€§èƒ½

2. **æ‰©å±•åˆ°å…¶ä»–æ¨¡å‹**ï¼š
   - GPT ç³»åˆ—æ¨¡å‹
   - BERT å’Œ encoder-only æ¨¡å‹
   - å¤šæ¨¡æ€æ¨¡å‹

3. **ä¼˜åŒ–é€šä¿¡**ï¼š
   - å®ç°æ¢¯åº¦ç´¯ç§¯
   - å°è¯•æ··åˆç²¾åº¦è®­ç»ƒ
   - ä¼˜åŒ– all-reduce æ¨¡å¼