# å¦‚ä½•åœ¨ JAX ä¸­è¿è¡Œ Hugging Face æ¨¡å‹ï¼ˆç¬¬ä¸‰éƒ¨åˆ†ï¼‰

åœ¨ä¹‹å‰çš„æ–‡ç« ä¸­ï¼ˆ[ç¬¬ä¸€éƒ¨åˆ†](01-run-huggingface-model-in-jax-zh.md) å’Œ [ç¬¬äºŒéƒ¨åˆ†](02-run-huggingface-model-distributed-zh.md)ï¼‰ï¼Œæˆ‘ä»¬æ¢è®¨äº†å¦‚ä½•è°ƒç”¨ HuggingFace æ¨¡å‹çš„ `forward` å‡½æ•°ã€‚ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•è¿è¡Œå…¶è‡ªå›å½’è§£ç å‡½æ•°ã€‚ä½†åœ¨æ­¤ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆæ·±å…¥äº†è§£ `torchax` çš„å·¥ä½œåŸç†ã€‚

åœ¨å¼€å§‹ä¹‹å‰ï¼Œå¦‚æœæˆ‘ä»¬æŒ‰ç…§ä¹‹å‰çš„ç¤ºä¾‹å®‰è£…äº† `torchax`ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼ä» GitHub é‡æ–°å®‰è£…ï¼š
transformers éœ€è¦ç‰¹åœ°ç‰ˆæœ¬

```bash
pip install git+https://github.com/google/torchax.git
pip install transformers==4.48.0
```

å› ä¸ºåœ¨ç¼–å†™æœ¬æ–‡æ—¶å‘ç°äº†ä¸€äº›æœ€è¿‘çš„ bug ä¿®å¤ã€‚

-----

## torchax çš„å·¥ä½œåŸç†

### è¡¨é¢ç°è±¡ vs å®é™…æœºåˆ¶

`torchax` çœ‹èµ·æ¥åƒæ˜¯åœ¨å°† PyTorch æ¨¡å‹è½¬æ¢ä¸º JAX å‡½æ•°ï¼Œä½†å®é™…ä¸Šå®ƒåšçš„æ˜¯ä¸åŒçš„äº‹æƒ…ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒç”¨è‡ªå®šä¹‰åŒ…è£…å™¨è£…é¥° JAX æ•°ç»„ï¼Œä½¿å…¶çœ‹èµ·æ¥åƒ `torch.Tensor`ã€‚

ç„¶åï¼Œå½“è°ƒç”¨ `torch.nn.Module` æ—¶ï¼Œå®ƒè®¤ä¸ºæ¥æ”¶çš„æ˜¯ `torch.Tensor` ä½œä¸ºè¾“å…¥ï¼Œä½†æˆ‘ä»¬å®é™…ä¸Šå·å·æ”¾è¿›å»çš„æ˜¯ `jax.Array`ï¼

![æœ¨é©¬ç­–ç•¥](image-trojan.png)

è¿™å°±åƒç‰¹æ´›ä¼Šæœ¨é©¬ä¸€æ ·â€”â€”å¤–è¡¨çœ‹èµ·æ¥æ˜¯ PyTorch Tensorï¼Œå†…éƒ¨å®é™…ä¸Šæ˜¯ JAX Arrayï¼

### æ·±å…¥ç†è§£ï¼šEnvironment å’Œ Tensor

ç°åœ¨è®©æˆ‘ä»¬ä¸ä½¿ç”¨ `extract_jax` APIï¼Œè€Œæ˜¯ç›´æ¥ä½¿ç”¨ `torchax` çš„ `Environment` å’Œ `Tensor` æ¥çœ‹çœ‹åˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆã€‚

```python
import torch
import torchax as tx
import jax
import jax.numpy as jnp

# torchax ç¯å¢ƒä½¿ PyTorch æ“ä½œèƒ½å¤Ÿåœ¨ torchax çš„ Tensor ä¸Šå·¥ä½œ
env = tx.default_env()

# ä» JAX æ•°ç»„å¼€å§‹ï¼š
arr = jnp.ones((4, 4))

# ç›´æ¥åœ¨ JAX æ•°ç»„ä¸Šè°ƒç”¨ torch å‡½æ•°ä¼šå‡ºé”™
# torch.matmul(arr, arr)  # è¿™ä¼šæŠ¥é”™ï¼

# å°† arr è½¬æ¢ä¸º Tensor
tensor = tx.interop.torch_view(arr)

print(isinstance(tensor, torch.Tensor))  # æ‰“å° True
print(tensor.__dict__)
```

è¿è¡Œç»“æœï¼š

```
Is torch Tensor: True
inner data of my tensor {'_elem': Array([[1., 1., 1., 1.],
       [1., 1., 1., 1.],
       [1., 1., 1., 1.],
       [1., 1., 1., 1.]], dtype=float32), '_env': <torchax.tensor.Environment object at 0x772f8cd67fd0>}
```

### è§£æç»“æœ

ä»è¿™é‡Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼š
1. ä» JAX Array è½¬æ¢çš„ tensor **ç¡®å®æ˜¯** `torch.Tensor` ç±»å‹
2. ä½†å®ƒå†…éƒ¨æŒæœ‰çš„æ˜¯æˆ‘ä»¬åŸå§‹çš„ JAX Arrayï¼ˆå­˜å‚¨åœ¨ `_elem` å­—æ®µä¸­ï¼‰
3. å®ƒè¿˜è®°ä½äº†æ‰€å±çš„ç¯å¢ƒå¯¹è±¡ï¼ˆé»˜è®¤æƒ…å†µä¸‹ä¸ `tx.default_env()` ç›¸åŒï¼‰

### åœ¨ torchax Tensor ä¸Šè¿è¡Œ PyTorch æ“ä½œ

ç°åœ¨æˆ‘ä»¬å¯ä»¥å°è¯•åœ¨è¿™ä¸ª tensor ä¸Šè¿è¡Œä¸€äº› PyTorch æ“ä½œã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦æ¿€æ´»ç¯å¢ƒï¼Œç„¶ååœ¨å…¶ä¸­è¿è¡Œæ“ä½œï¼š

```python
with env:
  print(torch.matmul(tensor, tensor))
  print(torch.sin(tensor))
  print(torch.exp(tensor))
```

æˆ‘ä»¬å¾—åˆ°ç»“æœã€‚æ³¨æ„è¿™ä¸æ˜¯å¸¸è§„çš„ `torch.Tensor`ï¼Œè€Œæ˜¯å†…éƒ¨æœ‰ `jax.Array` çš„é‚£ç§ï¼š

```
Tensor(<class 'jaxlib._jax.ArrayImpl'> [[4. 4. 4. 4.]
 [4. 4. 4. 4.]
 [4. 4. 4. 4.]
 [4. 4. 4. 4.]])
Tensor(<class 'jaxlib._jax.ArrayImpl'> [[0.841471 0.841471 0.841471 0.841471]
 [0.841471 0.841471 0.841471 0.841471]
 [0.841471 0.841471 0.841471 0.841471]
 [0.841471 0.841471 0.841471 0.841471]])
Tensor(<class 'jaxlib._jax.ArrayImpl'> [[2.7182817 2.7182817 2.7182817 2.7182817]
 [2.7182817 2.7182817 2.7182817 2.7182817]
 [2.7182817 2.7182817 2.7182817 2.7182817]
 [2.7182817 2.7182817 2.7182817 2.7182817]])
```

### åˆ›å»º torchax Tensor çš„å¦ä¸€ç§æ–¹å¼

é™¤äº†é€šè¿‡åŒ…è£… JAX Array æ¥åˆ›å»º `torchax.Tensor`ï¼Œå¦ä¸€ç§æ–¹æ³•æ˜¯åœ¨å¸¸è§„ `torch.Tensor`ï¼ˆåœ¨ CPU ä¸Šï¼‰ä¸Šè°ƒç”¨ `.to('jax')`ã€‚

å› æ­¤ï¼Œç¼–å†™ä¸Šè¿°ç¤ºä¾‹çš„å¦ä¸€ç§æ–¹å¼æ˜¯ï¼š

```python
with env:
  tensor = torch.ones((4,4)).to('jax')
  print(torch.matmul(tensor, tensor))
  print(torch.sin(tensor))
  print(torch.exp(tensor))
```

### è¿è¡Œå®Œæ•´ç¤ºä¾‹

æ‚¨å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤é‡ç°ä¸Šè¿°ç¤ºä¾‹ï¼š

```bash
python torchax-demo.py
```

### æ ¸å¿ƒåŸç†æ€»ç»“

**æœºå™¨å­¦ä¹ æ¨¡å‹æ˜¯ç”± torch æ“ä½œç»„æˆçš„è®¡ç®—å›¾ã€‚å› æ­¤ï¼Œå¦‚æœæ¯ä¸ª torch æ“ä½œéƒ½åœ¨æˆ‘ä»¬çš„ Tensor å˜ä½“ä¸Šè¿è¡Œï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å®ç°äº†åœ¨ JAX ä¹‹ä¸Šè¿è¡Œ torch æ¨¡å‹ã€‚**

è¿™æ˜¯ä¸€ä¸ªä¼˜é›…çš„è®¾è®¡ï¼š
1. **æ— éœ€ä¿®æ”¹æ¨¡å‹ä»£ç **ï¼šæ¨¡å‹ä»ç„¶ä½¿ç”¨æ ‡å‡†çš„ PyTorch API
2. **é€æ˜çš„åç«¯åˆ‡æ¢**ï¼šJAX ä½œä¸ºåç«¯æ‰§è¡Œå®é™…è®¡ç®—
3. **æœ€ä½³çš„ä¸¤ä¸ªä¸–ç•Œ**ï¼šPyTorch çš„æ˜“ç”¨æ€§ + JAX çš„æ€§èƒ½

-----

## ä½¿ç”¨æ–° API é‡å†™ä¹‹å‰çš„ç¤ºä¾‹

ç°åœ¨ï¼Œè®©æˆ‘ä»¬é€šè¿‡æ„é€  tensor å’Œè°ƒç”¨ torch æ¨¡å‹æ¥é‡å†™æˆ‘ä»¬åœ¨ä¹‹å‰æ–‡ç« ä¸­çœ‹åˆ°çš„ç¤ºä¾‹ã€‚

```python
# å°†æ¨¡å‹çš„æƒé‡ç§»åŠ¨åˆ° 'jax' è®¾å¤‡ï¼Œå³ä¸€ä¸ªå†…éƒ¨æœ‰ JAX æ•°ç»„çš„ tensor
with env:
  model.to('jax')
  weights = shard_weights_llama(mesh, model.state_dict())
  input_ids = model_inputs.input_ids.to('jax').apply_jax_(
    jax.device_put,
    NamedSharding(mesh, P()))
  tx.interop.call_jax(jax.block_until_ready, weights)
  print(model(input_ids))
```

### é€æ­¥è§£æ

è®©æˆ‘ä»¬è¯¦ç»†åˆ†æä¸Šé¢å‘ç”Ÿçš„äº‹æƒ…ï¼š

1. **`model.to('jax')`**ï¼š
   - å°† torch æ¨¡å‹çš„æƒé‡ç§»åŠ¨åˆ°ç‰¹æ®Šçš„ 'jax' è®¾å¤‡
   - ç±»ä¼¼äº `model.to('cuda')` ä½¿ç”¨ CUDA åç«¯
   - ä¸€æ—¦å‘ç”Ÿè¿™ç§æƒ…å†µï¼Œtensor ç±»å‹å°†å˜ä¸º `torchax.Tensor`
   - è¿™ä¸ª tensor ç±»æœ‰ä¸€ä¸ªé¢å¤–çš„æ–¹æ³•ï¼š`apply_jax_`ï¼Œå®ƒå°†ä»»ä½• JAX å‡½æ•°åº”ç”¨äºå†…éƒ¨çš„ JAX æ•°ç»„

2. **æƒé‡åˆ†ç‰‡**ï¼š
   - æ¨¡å‹ä¸­çš„æƒé‡ä»ç„¶æ˜¯æœªåˆ†ç‰‡çš„
   - æ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨ä¸Šæ¬¡ä½¿ç”¨çš„ç›¸åŒåˆ†ç‰‡æ–¹æ³•å¯¹å®ƒä»¬è¿›è¡Œåˆ†ç‰‡

3. **è°ƒç”¨æ¨¡å‹**ï¼š
   - æˆ‘ä»¬åƒè°ƒç”¨ä»»ä½• PyTorch æ¨¡å‹ä¸€æ ·è°ƒç”¨æ¨¡å‹
   - å¾—åˆ°é¢„æœŸçš„ç»“æœ

### æ‰§è¡Œç»“æœ

```
CausalLMOutputWithPast(loss=None, logits=Tensor(<class 'jaxlib._jax.ArrayImpl'> [[[-12.950611    -7.4854484   -0.42371067 ...  -6.819363    -8.073828
    -7.5583534 ]
  [-13.508438   -11.716616    -6.9578876  ...  -9.135823   -10.237023
    -8.56888   ]
  [-12.8517685  -11.180469    -4.0543456  ...  -7.9564795  -11.546011
   -10.686134  ]
  ...
  [ -2.983235    -5.621302    11.553352   ...  -2.6286669   -2.8319468
    -1.9902805 ]
  [ -8.674949   -10.042385     3.4400458  ...  -3.7776647   -8.616567
    -5.7228904 ]
  [ -4.0748825   -4.706395     5.117742   ...   6.7174563    0.5748794
     2.506649  ]]]), past_key_values=DynamicCache(), hidden_states=None, attentions=None)
```

å®Œç¾ï¼æˆ‘ä»¬å¾—åˆ°äº†ä¸ä¹‹å‰ç›¸åŒçš„ç»“æœã€‚

-----

## ç”¨å½¢çŠ¶è§£é‡Šè‡ªå›å½’è§£ç 

### LLM çš„åŸºæœ¬å·¥ä½œåŸç†

LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰è¢«è®­ç»ƒæ¥é¢„æµ‹ç»™å®šè¾“å…¥å¥å­çš„ä¸‹ä¸€ä¸ª tokenã€‚

![LLM é¢„æµ‹](llm-predict.png)

### è¾“å…¥è¾“å‡ºå½¢çŠ¶

ç»™å®šé•¿åº¦ä¸º `n` çš„è¾“å…¥åºåˆ—ï¼š
- **è¾“å…¥**ï¼šå½¢çŠ¶ä¸º `(1, n)` çš„å¼ é‡ï¼ˆå…¶ä¸­ 1 æ˜¯ batch sizeï¼‰
- **è¾“å‡º**ï¼šä¹Ÿæ˜¯å½¢çŠ¶ä¸º `(1, n)` çš„å¼ é‡ï¼Œç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ª token
- **å…³æ³¨ç‚¹**ï¼šæˆ‘ä»¬åªå…³å¿ƒæœ€åä¸€ä¸ª token çš„é¢„æµ‹

### è¿­ä»£è¿‡ç¨‹

ä¸‹ä¸€æ­¥æ˜¯å°†è¿™ä¸ª token é™„åŠ åˆ°åŸå§‹è¾“å…¥ï¼Œå½¢æˆå½¢çŠ¶ä¸º `(1, n + 1)` çš„è¾“å…¥åºåˆ—ï¼Œç„¶åæˆ‘ä»¬é‡å¤è¿™ä¸ªè¿‡ç¨‹ `m` æ¬¡è¿­ä»£ï¼Œæˆ–è€…ç›´åˆ°æ¨¡å‹äº§ç”Ÿåœæ­¢ä¿¡å·ï¼Œé€šå¸¸æ˜¯ `å¥å­ç»“æŸï¼ˆeosï¼‰` tokenã€‚

æ¢å¥è¯è¯´ï¼ŒLLM çš„è¾“å…¥å’Œè¾“å‡ºå½¢çŠ¶æ˜¯ï¼š

```
è¿­ä»£ 1: (1, n)     -> (1, n)
è¿­ä»£ 2: (1, n + 1) -> (1, n + 1)
è¿­ä»£ 3: (1, n + 2) -> (1, n + 2)
...
```

**å…³é”®è§‚å¯Ÿ**ï¼šæ¯æ¬¡è¿­ä»£çš„å½¢çŠ¶éƒ½åœ¨å˜åŒ–ã€‚

### å½¢çŠ¶å˜åŒ–çš„é—®é¢˜

è¿™ç§åŠ¨æ€å½¢çŠ¶å˜åŒ–å¯¹ JAX çš„ JIT ç¼–è¯‘æå‡ºäº†æŒ‘æˆ˜ï¼š
- JAX çš„ JIT éœ€è¦å›ºå®šçš„è¾“å…¥å½¢çŠ¶
- æ¯æ¬¡å½¢çŠ¶æ”¹å˜éƒ½ä¼šè§¦å‘é‡æ–°ç¼–è¯‘
- è¿™ä¼šä¸¥é‡é™ä½æ€§èƒ½

-----

## ä½¿ç”¨ KV Cache è¿›è¡Œè§£ç 

### ä¸ºä»€ä¹ˆéœ€è¦ KV Cacheï¼Ÿ

ç†Ÿæ‚‰ LLM æ¨ç†çš„äººä¼šæŒ‡å‡ºï¼Œé€šå¸¸çš„è‡ªå›å½’è§£ç è®¾ç½®ä¼šä½¿ç”¨ `KVCache`ã€‚[è¿™ç¯‡æ–‡ç« ](https://medium.com/@plienhar/llm-inference-series-4-kv-caching-a-deeper-look-4ba9a77746c8) å¯¹æ­¤æœ‰å¾ˆå¥½çš„è§£é‡Šã€‚

### KV Cache çš„æ ¸å¿ƒæ€æƒ³

ä¸»è¦æ€æƒ³æ˜¯ï¼š
- **è§‚å¯Ÿ**ï¼šè¿­ä»£ 1 åªäº§ç”Ÿäº†ä¸€ä¸ªæ–° token
- **å…³é”®**ï¼šè¿™æ˜¯æ¨¡å‹å”¯ä¸€æ²¡è§è¿‡çš„ token
- **ä¼˜åŒ–**ï¼šæˆ‘ä»¬å¯ä»¥å°†ä¹‹å‰è§è¿‡çš„ token ç¼–ç åˆ°ç¼“å­˜ä¸­å¹¶é‡ç”¨ä¸€äº›è®¡ç®—

### ç®€å•ç±»æ¯”

æƒ³è±¡ä½ åœ¨è¯»ä¸€æœ¬ä¹¦ï¼š
- **æ²¡æœ‰ç¼“å­˜**ï¼šæ¯æ¬¡è¯»æ–°æ®µè½æ—¶ï¼Œéƒ½è¦é‡æ–°ç†è§£æ•´æœ¬ä¹¦
- **æœ‰ç¼“å­˜**ï¼šä½ è®°ä½äº†ä¹‹å‰è¯»è¿‡çš„å†…å®¹ï¼Œåªéœ€ç†è§£æ–°æ®µè½

### KV Cache çš„è¾“å…¥è¾“å‡º

åœ¨ä½¿ç”¨ KVCache çš„æ¨ç†è®¾ç½®ä¸­ï¼Œæ¨¡å‹çš„è¾“å…¥å’Œè¾“å‡ºå¤§è‡´å¦‚ä¸‹ï¼š

```
è¿­ä»£ 1: (1, n)          -> (1, n),     kvcache(n)
è¿­ä»£ 2: (1, 1), kvcache(n)   -> (1, 1),     kvcache(n + 1)
è¿­ä»£ 3: (1, 1), kvcache(n + 1) -> (1, 1),     kvcache(n + 2)
...
```

### KV Cache çš„ç»“æ„

è¿™é‡Œæˆ‘ä½¿ç”¨ç¬¦å· `kvcache(n)` æ¥è¡¨ç¤ºåºåˆ—é•¿åº¦ä¸º `n` çš„ kvcacheã€‚

**å®Œæ•´å½¢çŠ¶**ï¼š
```
(batch_size, num_heads, sequence_length, head_dim) Ã— num_layers Ã— 2
```

**ä¸ºä»€ä¹ˆä¹˜ä»¥ 2ï¼Ÿ**
- æ¯å±‚æœ‰ Key å’Œ Value ä¸¤ä¸ªç¼“å­˜
- Kï¼ˆKeyï¼‰å’Œ Vï¼ˆValueï¼‰åˆ†åˆ«å­˜å‚¨

### å®é™…æ£€æŸ¥ KV Cache çš„å½¢çŠ¶

è®©æˆ‘ä»¬å®é™…è¿è¡Œæ¨¡å‹å¹¶æ£€æŸ¥ KV cache çš„å½¢çŠ¶ï¼š

```python
print('number of layers', len(res[1]))
for k, v in res[1]:
  print(k.shape, v.shape)
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼š

```
number of layers 32
torch.Size([1, 32, 12, 128]) torch.Size([1, 32, 12, 128])
torch.Size([1, 32, 12, 128]) torch.Size([1, 32, 12, 128])
torch.Size([1, 32, 12, 128]) torch.Size([1, 32, 12, 128])
torch.Size([1, 32, 12, 128]) torch.Size([1, 32, 12, 128])
...
```

### è§£é‡Š KV Cache ç»´åº¦

å¯¹äº Llama-2 æ¨¡å‹ï¼š
- **å±‚æ•°**ï¼š32 å±‚
- **æ³¨æ„åŠ›å¤´æ•°**ï¼š32 ä¸ªå¤´
- **å½¢çŠ¶è§£é‡Š**ï¼š`[batch_size, num_heads, seq_len, head_dim]`
  - `1`ï¼šbatch size
  - `32`ï¼šæ³¨æ„åŠ›å¤´æ•°
  - `12`ï¼šå½“å‰åºåˆ—é•¿åº¦
  - `128`ï¼šæ¯ä¸ªå¤´çš„ç»´åº¦

### ä½¿ç”¨ KV Cache è¿›è¡Œè‡ªå›å½’è§£ç 

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¼ å›ä¸‹ä¸€ä¸ª token å’Œç¼“å­˜æ¥è¿›è¡Œè‡ªå›å½’è§£ç ï¼š

```python
print('number of layers', len(res[1]))
for k, v in res[1]:
  print('first kv cache')
  print(k.shape, v.shape)
  break

# è´ªå©ªé‡‡æ ·ï¼šé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ token
next_token = torch.argmax(res[0][:, -1], dim=-1)

# ä½¿ç”¨ç¼“å­˜è¿›è¡Œä¸‹ä¸€æ¬¡é¢„æµ‹
res = model(next_token.unsqueeze(0), past_key_values=res[1])
print('number of layers', len(res[1]))
for k, v in res[1]:
  print('second kv cache')
  print(k.shape, v.shape)
  break
```

### é‡‡æ ·ç­–ç•¥è¯´æ˜

**è´ªå©ªé‡‡æ ·ï¼ˆGreedy Samplingï¼‰**ï¼š
- æ€»æ˜¯é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ token
- ç®€å•ä½†å¯èƒ½ä¸å¤Ÿå¤šæ ·åŒ–

**å…¶ä»–é‡‡æ ·ç­–ç•¥**ï¼š
```python
# Top-k é‡‡æ ·
top_k_logits, top_k_indices = torch.topk(logits, k=50)
probs = torch.softmax(top_k_logits, dim=-1)
next_token = top_k_indices[torch.multinomial(probs, 1)]

# Top-p (nucleus) é‡‡æ ·
sorted_logits, sorted_indices = torch.sort(logits, descending=True)
cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
# ... (é€‰æ‹©ç´¯ç§¯æ¦‚ç‡åœ¨ p å†…çš„ tokens)

# æ¸©åº¦é‡‡æ ·
temperature = 0.7
scaled_logits = logits / temperature
probs = torch.softmax(scaled_logits, dim=-1)
next_token = torch.multinomial(probs, 1)
```

### Cache å¢é•¿éªŒè¯

è¾“å‡ºç»“æœï¼š

```
number of layers 32
first kv cache
torch.Size([1, 32, 12, 128]) torch.Size([1, 32, 12, 128])
number of layers 32
second kv cache
torch.Size([1, 32, 13, 128]) torch.Size([1, 32, 13, 128])
```

æˆ‘ä»¬çœ‹åˆ° dynamic cache çš„å¤§å°å¢é•¿äº† 1ã€‚

### Dynamic Cache ä¸ JAX JIT

**é—®é¢˜**ï¼š
- `jax.jit` ä¸å–œæ¬¢å˜åŒ–çš„å½¢çŠ¶ï¼ˆä¼šé‡æ–°ç¼–è¯‘ï¼ï¼‰
- å¦‚æœæˆ‘ä»¬ç»§ç»­ä½¿ç”¨ `DynamicCache`ï¼Œåªèƒ½ä½¿ç”¨ eager æ¨¡å¼è¿›è¡Œæ¨ç†

**è§£å†³æ–¹æ¡ˆé¢„å‘Š**ï¼š
- ä½¿ç”¨ `StaticCache`ï¼ˆå›ºå®šæœ€å¤§é•¿åº¦ï¼‰
- é¿å…é‡æ–°ç¼–è¯‘

### å®ç°å®Œæ•´çš„è‡ªå›å½’è§£ç 

```python
def autoregressive_decode(model, input_ids, tokenizer, max_tokens=50):
  """ä½¿ç”¨ Dynamic Cache çš„è‡ªå›å½’è§£ç ï¼ˆeager æ¨¡å¼ï¼‰"""
  start = time.perf_counter()
  
  # ç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­
  res = model(input_ids)
  next_token = torch.argmax(res[0][:, -1], dim=-1)
  result_tokens = [int(next_token.item())]

  # è¿­ä»£ç”Ÿæˆ
  for _ in range(max_tokens):
    res = model(next_token.unsqueeze(0), past_key_values=res[1])
    next_token = torch.argmax(res[0][:, -1], dim=-1)
    if next_token.item() == tokenizer.eos_token:
      break
    result_tokens.append(next_token.item())
  
  end = time.perf_counter()
  
  print('decoded', tokenizer.batch_decode([result_tokens]))
  print(f'took {end - start} seconds')
  return result_tokens
```

### æ‰§è¡Œç»“æœ

```
decoded ['100% in the ingredients.\nI've been baking cakes for as long as I can remember. I've always loved the process of baking and the smell of freshly baked cakes.\nI']
took 130.90283443999942 seconds
```

**åˆ†æ**ï¼š
- âœ… æ¨¡å‹æˆåŠŸç”Ÿæˆäº†æ–‡æœ¬
- âŒ 130 ç§’å®Œæˆä¸€ä¸ªè¯·æ±‚å¤ªæ…¢äº†
- ğŸ’¡ éœ€è¦ä½¿ç”¨ `jax.jit` æ¥åŠ é€Ÿ

-----

## Static Cache å’Œ jax.jit

### Dynamic Cache çš„é—®é¢˜

`jax.jit` å’Œä¸Šé¢ä½¿ç”¨çš„ `DynamicCache` çš„é—®é¢˜æ˜¯ï¼š
- æ¯æ¬¡è¿­ä»£çš„è¾“å…¥å’Œè¾“å‡ºå½¢çŠ¶éƒ½åœ¨å˜åŒ–
- ç›²ç›®åº”ç”¨ `jax.jit` ä¼šæ¯” eager æ¨¡å¼æ›´æ…¢
- åŸå› ï¼šéœ€è¦é‡æ–°ç¼–è¯‘è®¡ç®—å›¾è¿è¡Œä¸€æ¬¡ï¼Œç„¶åä¸¢å¼ƒ

### Static Cache çš„å¼•å…¥

å¹¸è¿çš„æ˜¯ï¼ŒHuggingFace æœ‰ä¸€ä¸ªè®¾ç½®å¯ä»¥ä½¿ç”¨ `StaticCache`â€”â€”ä¸€ä¸ªå…·æœ‰å›ºå®šæœ€å¤§é•¿åº¦çš„ç¼“å­˜ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥é¿å…é‡æ–°ç¼–è¯‘ã€‚

æ ¹æ® [LLM æ¨ç†ä¼˜åŒ–](https://huggingface.co/docs/transformers/v4.44.0/en/llm_optims?static-kv=advanced+usage%3A+control+Static+Cache#static-kv-cache-and-torchcompile) æ–‡æ¡£ï¼Œ`StaticCache` æ­£æ˜¯ä¸ºäº†æ”¯æŒ `torch.compile` è€Œå¼•å…¥çš„ï¼›å®ƒä¹Ÿå–œæ¬¢é™æ€å½¢çŠ¶ã€‚

### Static Cache çš„å·¥ä½œåŸç†

```python
# åˆ›å»ºå›ºå®šå¤§å°çš„ç¼“å­˜
cache = StaticCache(
    config=model.config,
    max_batch_size=1,
    max_cache_len=100,  # å›ºå®šæœ€å¤§é•¿åº¦
    device='jax',
    dtype=model.dtype
)

# ä½¿ç”¨ç¼“å­˜
for i in range(max_tokens):
    # cache çš„å½¢çŠ¶å§‹ç»ˆæ˜¯ (1, num_heads, 100, head_dim)
    # ä½†åªæœ‰å‰ i ä¸ªä½ç½®æ˜¯æœ‰æ•ˆçš„
    logits, cache = model(token, past_key_values=cache, ...)
```

### å®ç° Static Cache è§£ç 

æˆ‘ä»¬ç¼–å†™ä»¥ä¸‹å‡½æ•°æ¥æµ‹è¯•ï¼š

**æ³¨æ„**ï¼šPython ä»£ç çœ‹èµ·æ¥æ›´å¤æ‚ï¼Œä½†å®ƒæ˜¯ä» HuggingFace çš„ [LLM æ¨ç†ä¼˜åŒ–](https://huggingface.co/docs/transformers/v4.44.0/en/llm_optims?static-kv=advanced+usage%3A+control+Static+Cache#static-kv-cache-and-torchcompile) æ–‡æ¡£ä¸­å¤åˆ¶çš„ã€‚

```python
from transformers.cache_utils import StaticCache

def autoregressive_decode_static(model, input_ids, tokenizer, max_tokens=50):
  """ä½¿ç”¨ Static Cache çš„è‡ªå›å½’è§£ç """
  
  def decode_one_tokens(cur_token, input_pos, cache_position, past_key_values):
    """è§£ç å•ä¸ª token"""
    logits, cache = model(
        cur_token,
        position_ids=input_pos,
        cache_position=cache_position,
        past_key_values=past_key_values,
        return_dict=False,
        use_cache=True
    )
    new_token = torch.argmax(logits[:, -1], dim=-1)[:, None]
    return new_token, cache

  batch_size, seq_length = input_ids.shape
  with torch.no_grad():
    start = time.perf_counter()
    
    # åˆ›å»º Static Cache
    past_key_values = StaticCache(
        config=model.config, 
        max_batch_size=1, 
        max_cache_len=max_tokens, 
        device='jax', 
        dtype=model.dtype
    )
    cache_position = torch.arange(seq_length, device='jax')
    generated_ids = []

    # ç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­ï¼ˆå¤„ç†æ•´ä¸ªè¾“å…¥åºåˆ—ï¼‰
    logits, past_key_values = model(
        input_ids, 
        cache_position=cache_position, 
        past_key_values=past_key_values, 
        return_dict=False, 
        use_cache=True
    )
    next_token = torch.argmax(logits[:, -1], dim=-1)[:, None]
    generated_ids.append(next_token[:, 0].item())

    # è‡ªå›å½’ç”Ÿæˆ
    cache_position = torch.tensor([seq_length + 1], device='jax')
    for _ in range(1, max_tokens):
        next_token, past_key_values = decode_one_tokens(
          next_token.clone(), None, cache_position, past_key_values)
        generated_ids.append(next_token.int().item())
        cache_position += 1
    
    end = time.perf_counter()

  text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
  print(text)
  print('Time: ', end - start)
```

### Static Cache ä»£ç è¯¦è§£

**å…³é”®å‚æ•°**ï¼š

1. **max_cache_len**ï¼š
   - é¢„åˆ†é…çš„ç¼“å­˜é•¿åº¦
   - å¿…é¡» >= å®é™…ç”Ÿæˆçš„åºåˆ—é•¿åº¦
   - æƒè¡¡ï¼šæ›´å¤§ = æ›´å¤šå†…å­˜ï¼Œæ›´å° = å¯èƒ½ä¸å¤Ÿç”¨

2. **cache_position**ï¼š
   - è·Ÿè¸ªå½“å‰å†™å…¥ç¼“å­˜çš„ä½ç½®
   - å…è®¸é‡ç”¨å›ºå®šå¤§å°çš„ç¼“å­˜

3. **position_ids**ï¼š
   - å‘Šè¯‰æ¨¡å‹ token çš„ä½ç½®
   - å¯¹ä½ç½®ç¼–ç å¾ˆé‡è¦

### æ‰§è¡Œç»“æœ

```
['1', '0', '0', '%', 'but', 'ter', '.', '\n', 'I', ''', 'm', 'not', 'sure', 'if', 'it', ''', 's', 'the', 'but', 'ter', 'or', 'the', 'eggs', ',', 'but', 'I', ''', 'm', 'pretty', 'sure', 'it', ''', 's', 'the', 'but', 'ter', '.', '\n', 'I', ''', 's', '\n', 'I', ''', 's', '\n', 'I', ''', '\n', 'I']
Time:  88.39702287199907
```

**åˆ†æ**ï¼š
- âœ… å¾—åˆ°äº†ç›¸åŒçš„è¾“å‡º
- âœ… æ›´å¿«çš„æ—¶é—´ï¼ˆ88 ç§’ vs 130 ç§’ï¼‰
- ğŸ’¡ è¿˜æ²¡æœ‰å°è¯•ç¼–è¯‘ï¼

-----

## ç°åœ¨è®©æˆ‘ä»¬è¿›è¡Œ JIT ç¼–è¯‘

### ä½¿ç”¨ torchax.interop.jax_jit

è¦ä½¿ç”¨ `jax.jit` ç¼–è¯‘å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `torchax.interop.jax_jit` è¾…åŠ©å‡½æ•°ã€‚

æˆ‘ä»¬å¯¹ä¸Šè¿°å‡½æ•°è¿›è¡Œä»¥ä¸‹æ›´æ”¹ï¼š

```python
# åœ¨å®šä¹‰ decode_one_tokens åæ·»åŠ è¿™ä¸€è¡Œ
jitted = tx.interop.jax_jit(decode_one_tokens)

# æ›¿æ¢è¿™ä¸€è¡Œ
- next_token, past_key_values = decode_one_tokens(
# ä¸ºè¿™ä¸€è¡Œï¼š
+ next_token, past_key_values = jitted(
```

æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬æ­£åœ¨ jit ç¼–è¯‘ `decode_one_tokens`ï¼Œå¹¶ç”¨ jitted å‡½æ•°æ›¿æ¢å¯¹å®ƒçš„è°ƒç”¨ã€‚

### jax_jit vs jax.jit

**ä¸ºä»€ä¹ˆä½¿ç”¨ `tx.interop.jax_jit` è€Œä¸æ˜¯ `jax.jit`ï¼Ÿ**

- `jax.jit` ä½œç”¨äº JAX å‡½æ•°ï¼ˆæ¥å—å’Œè¿”å› `jax.Array` çš„å‡½æ•°ï¼‰
- `tx.interop.jax_jit` ä½œç”¨äº torch å‡½æ•°ï¼ˆæ¥å—å’Œè¿”å› `torch.Tensor` çš„å‡½æ•°ï¼‰

```python
# JAX å‡½æ•°
def jax_func(x):  # x æ˜¯ jax.Array
    return jax.numpy.sin(x)  # è¿”å› jax.Array
jitted_jax = jax.jit(jax_func)

# Torch å‡½æ•°
def torch_func(x):  # x æ˜¯ torch.Tensor
    return torch.sin(x)  # è¿”å› torch.Tensor
jitted_torch = tx.interop.jax_jit(torch_func)
```

### é‡åˆ°çš„é”™è¯¯

è¿è¡Œæ—¶ï¼Œæˆ‘ä»¬å‘ç°äº†è¿™ä¸ªé”™è¯¯ï¼š

```
Traceback (most recent call last):
  File "/home/hanq_google_com/learning_machine/jax-huggingface/jax_hg_03.py", line 201, in <module>
    autoregressive_decode_static(model, input_ids, tokenizer)
  File "/home/hanq_google_com/learning_machine/jax-huggingface/jax_hg_03.py", line 177, in autoregressive_decode_static
    next_token, past_key_values = jitted(
  File "/home/hanq_google_com/pytorch/xla/torchax/torchax/interop.py", line 220, in call_jax
    res: JaxValue = jax_func(*args, **kwargs)
TypeError: Error interpreting argument to functools.partial(<function call_torch at 0x7d1cea648af0>, <function autoregressive_decode_static.<locals>.decode_one_tokens at 0x7d1c86d0e440>) as an abstract array. The problematic value is of type <class 'transformers.cache_utils.StaticCache'> and was passed to the function at path args[3].
This typically means that a jit-wrapped function was called with a non-array argument, and this argument was not marked as static using the static_argnums or static_argnames parameters of jax.jit.
```

### é”™è¯¯ 1ï¼šåˆæ˜¯ Pytree é—®é¢˜

å›æƒ³[ç¬¬ä¸€éƒ¨åˆ†](01-run-huggingface-model-in-jax-zh.md)ï¼Œæˆ‘ä»¬é‡åˆ°äº†å®Œå…¨ç›¸åŒçš„é—®é¢˜ï¼Œå³ `StaticCache` éœ€è¦åœ¨ pytree ä¸­æ³¨å†Œã€‚

è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š

```python
from jax.tree_util import register_pytree_node
from transformers import cache_utils

def _flatten_static_cache(cache):
  """å±•å¹³ Static Cache"""
  return (
      cache.key_cache,
      cache.value_cache,
  ), (cache._config, cache.max_batch_size, cache.max_cache_len)

def _unflatten_static_cache(aux, children):
  """é‡å»º Static Cache"""
  cache = cache_utils.StaticCache(*aux)
  cache._config = aux[0]
  cache.key_cache, cache.value_cache = children
  return cache

register_pytree_node(
  cache_utils.StaticCache,
  _flatten_static_cache,
  _unflatten_static_cache,
)
```

### Pytree æ³¨å†Œè¯¦è§£

**children vs aux_data**ï¼š
- **children**ï¼šéœ€è¦è¢« JAX è½¬æ¢çš„æ•°æ®ï¼ˆå¦‚ Arraysï¼‰
  - `key_cache`, `value_cache`
- **aux_data**ï¼šé™æ€å…ƒæ•°æ®ï¼Œä¸ä¼šè¢«è½¬æ¢
  - `_config`, `max_batch_size`, `max_cache_len`

### ä¸‹ä¸€ä¸ªé”™è¯¯ï¼šæ•è·çš„å¸¸é‡

å†æ¬¡è¿è¡Œï¼Œä¼¼ä¹å¡ä½äº†ï¼Œå‡ºç°ä»¥ä¸‹æ¶ˆæ¯ï¼š

```
/home/hanq_google_com/venv/lib/python3.10/site-packages/jax/_src/interpreters/mlir.py:1135: UserWarning: A large amount of constants were captured during lowering (13.48GB total). If this is intentional, disable this warning by setting JAX_CAPTURED_CONSTANTS_WARN_BYTES=-1. To obtain a report of where these constants were encountered, set JAX_CAPTURED_CONSTANTS_REPORT_FRAMES=-1.
```

### ç†è§£å¸¸é‡æ•è·é—®é¢˜

**å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ**

å½“æ‚¨ä½¿ç”¨ `jax.jit` æ—¶ï¼Œä»»ä½•åœ¨å‡½æ•°ä¸­ä½¿ç”¨ä½†**ä¸ä½œä¸ºè¾“å…¥å‚æ•°ä¼ é€’**çš„æ•°æ®éƒ½å°†ä½œä¸ºå¸¸é‡å†…è”åˆ°è®¡ç®—å›¾ä¸­ã€‚

**ä¸ºä»€ä¹ˆè¿™æ˜¯ä¸ªé—®é¢˜ï¼Ÿ**
1. **å¤§å¸¸é‡ä½¿è®¡ç®—å›¾å˜å¤§**
2. **å¯èƒ½ä½¿ç¼–è¯‘æ—¶é—´æ›´é•¿**
3. **æœ‰æ—¶ä¼šå¯¼è‡´æŒ‡ä»¤ç¼“å­˜ OOM**

### è¯†åˆ«é—®é¢˜

æˆ‘ä»¬åªæœ‰ä¸€ä¸ªåº”ç”¨äº† `jax.jit`ï¼ˆé€šè¿‡ `tx.interop.jax_jit`ï¼‰çš„å‡½æ•°ï¼š

```python
def decode_one_tokens(cur_token, input_pos, cache_position, past_key_values):
```

ä»”ç»†æŸ¥çœ‹ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°**æ¨¡å‹æƒé‡**ï¼ˆä¸€å¤§å—æ•°æ®ï¼‰æ²¡æœ‰åˆ—åœ¨è¾“å…¥å‚æ•°ä¸­ã€‚è®©æˆ‘ä»¬ä¿®å¤å®ƒã€‚

### è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨ torch.func.functional_call

å°† `decode_one_tokens` æ›¿æ¢ä¸ºï¼š

```python
def decode_one_tokens(model_weights, cur_token, input_pos, cache_position, past_key_values):
  """
  è§£ç å•ä¸ª token
  
  å‚æ•°:
    model_weights: æ¨¡å‹æƒé‡ï¼ˆä½œä¸ºè¾“å…¥ä¼ é€’ä»¥é¿å…å¸¸é‡æ•è·ï¼‰
    cur_token: å½“å‰ token
    input_pos: ä½ç½® ID
    cache_position: ç¼“å­˜ä½ç½®
    past_key_values: KV cache
  """
  logits, cache = torch.func.functional_call(
      model, 
      model_weights,  # æƒé‡ state_dict
      (cur_token,),   # args ä½œä¸ºå…ƒç»„
      dict(
        position_ids=input_pos,
        cache_position=cache_position,
        past_key_values=past_key_values,
        return_dict=False,
        use_cache=True) # kwargs ä½œä¸ºå­—å…¸
  )
  new_token = torch.argmax(logits[:, -1], dim=-1)[:, None]
  return new_token, cache
```

### torch.func.functional_call è¯¦è§£

[`torch.func.functional_call`](https://docs.pytorch.org/docs/stable/generated/torch.func.functional_call.html) å…è®¸æˆ‘ä»¬ï¼š

1. **ä¸´æ—¶æ›¿æ¢æ¨¡å‹æƒé‡**ï¼š
   ```python
   # ä¸ä¿®æ”¹åŸå§‹æ¨¡å‹
   output = torch.func.functional_call(model, new_weights, inputs)
   ```

2. **æ— çŠ¶æ€è°ƒç”¨**ï¼š
   - æ¨¡å‹çš„ `forward` æ–¹æ³•è¢«è°ƒç”¨
   - ä½†ä½¿ç”¨æä¾›çš„æƒé‡è€Œä¸æ˜¯ `model.parameters()`

3. **å¯¹ JIT å‹å¥½**ï¼š
   - æƒé‡ä½œä¸ºæ˜¾å¼è¾“å…¥
   - ä¸ä¼šè¢«æ•è·ä¸ºå¸¸é‡

### æ›´æ–°è°ƒç”¨ä»£ç 

ç°åœ¨è°ƒç”¨æ—¶éœ€è¦ä¼ é€’ `model_weights`ï¼š

```python
# è·å–æ¨¡å‹æƒé‡
model_weights = model.state_dict()

# åœ¨å¾ªç¯ä¸­ä½¿ç”¨
for _ in range(1, max_tokens):
    next_token, past_key_values = jitted(
        model_weights,  # æ·»åŠ æƒé‡å‚æ•°
        next_token.clone(), 
        None, 
        cache_position, 
        past_key_values
    )
    generated_ids.append(next_token.int().item())
    cache_position += 1
```

### æœ€ç»ˆæ€§èƒ½ç»“æœ

å†æ¬¡è¿è¡Œï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š

```
['1', '0', '0', '%', 'but', 'ter', '.', '\n', 'I', ''', 'm', 'not', 'sure', 'if', 'it', ''', 's', 'the', 'but', 'ter', 'or', 'the', 'eggs', ',', 'but', 'I', ''', 'm', 'pretty', 'sure', 'it', ''', 's', 'the', 'but', 'ter', '.', '\n', 'I', ''', 's', '\n', 'I', ''', 's', '\n', 'I', ''', '\n', 'I']
Time:  14.7717966591008
```

**æ€§èƒ½å¯¹æ¯”**ï¼š
- Dynamic Cache (eager): 130 ç§’
- Static Cache (eager): 88 ç§’
- Static Cache (JIT): **14.8 ç§’** âš¡

**åŠ é€Ÿæ¯”**ï¼š
- ç›¸æ¯” Dynamic Cache: 8.8x
- ç›¸æ¯” Static Cache eager: 6x

è¿™å¿«å¤šäº†ï¼å®Œæ•´çš„é‡ç°ä½äº [jax_hg_03.py](jax_hg_03.py)ã€‚

-----

## æ·±å…¥ç†è§£ï¼šé‡‡æ ·ç­–ç•¥

### è´ªå©ªé‡‡æ ·çš„å±€é™æ€§

æˆ‘ä»¬ä¸€ç›´ä½¿ç”¨è´ªå©ªé‡‡æ ·ï¼ˆ`argmax`ï¼‰ï¼Œä½†è¿™æœ‰å±€é™æ€§ï¼š
- æ€»æ˜¯é€‰æ‹©æœ€å¯èƒ½çš„ token
- ç¼ºä¹å¤šæ ·æ€§
- å¯èƒ½äº§ç”Ÿé‡å¤çš„æ–‡æœ¬

### Top-k é‡‡æ ·

```python
def top_k_sampling(logits, k=50, temperature=1.0):
    """
    Top-k é‡‡æ ·ï¼šä»æ¦‚ç‡æœ€é«˜çš„ k ä¸ª token ä¸­é‡‡æ ·
    
    å‚æ•°:
        logits: æ¨¡å‹è¾“å‡ºçš„ logits
        k: ä¿ç•™çš„ top token æ•°é‡
        temperature: æ¸©åº¦å‚æ•°ï¼ˆè¶Šé«˜è¶Šéšæœºï¼‰
    """
    # 1. åº”ç”¨æ¸©åº¦
    logits = logits / temperature
    
    # 2. è·å– top k
    top_k_logits, top_k_indices = torch.topk(logits, k)
    
    # 3. è®¡ç®—æ¦‚ç‡
    probs = torch.softmax(top_k_logits, dim=-1)
    
    # 4. é‡‡æ ·
    sampled_index = torch.multinomial(probs, 1)
    next_token = top_k_indices[sampled_index]
    
    return next_token

# ä½¿ç”¨ç¤ºä¾‹
next_token = top_k_sampling(logits[:, -1], k=50, temperature=0.8)
```

### Top-p (Nucleus) é‡‡æ ·

```python
def top_p_sampling(logits, p=0.9, temperature=1.0):
    """
    Top-p (nucleus) é‡‡æ ·ï¼šä»ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° p çš„æœ€å° token é›†åˆä¸­é‡‡æ ·
    
    å‚æ•°:
        logits: æ¨¡å‹è¾“å‡ºçš„ logits
        p: ç´¯ç§¯æ¦‚ç‡é˜ˆå€¼
        temperature: æ¸©åº¦å‚æ•°
    """
    # 1. åº”ç”¨æ¸©åº¦
    logits = logits / temperature
    
    # 2. æ’åº
    sorted_logits, sorted_indices = torch.sort(logits, descending=True)
    
    # 3. è®¡ç®—ç´¯ç§¯æ¦‚ç‡
    probs = torch.softmax(sorted_logits, dim=-1)
    cumulative_probs = torch.cumsum(probs, dim=-1)
    
    # 4. ç§»é™¤ç´¯ç§¯æ¦‚ç‡è¶…è¿‡ p çš„ token
    sorted_indices_to_remove = cumulative_probs > p
    # ä¿æŒè‡³å°‘ä¸€ä¸ª token
    sorted_indices_to_remove[..., 0] = False
    
    # 5. åˆ›å»ºæ©ç 
    indices_to_remove = sorted_indices_to_remove.scatter(
        -1, sorted_indices, sorted_indices_to_remove
    )
    logits[indices_to_remove] = float('-inf')
    
    # 6. é‡‡æ ·
    probs = torch.softmax(logits, dim=-1)
    next_token = torch.multinomial(probs, 1)
    
    return next_token

# ä½¿ç”¨ç¤ºä¾‹
next_token = top_p_sampling(logits[:, -1], p=0.9, temperature=0.8)
```

### ç»„åˆç­–ç•¥

```python
def combined_sampling(logits, top_k=50, top_p=0.9, temperature=0.8):
    """ç»„åˆ top-k å’Œ top-p é‡‡æ ·"""
    # 1. æ¸©åº¦
    logits = logits / temperature
    
    # 2. Top-k è¿‡æ»¤
    top_k_logits, top_k_indices = torch.topk(logits, top_k)
    
    # 3. Top-p è¿‡æ»¤ï¼ˆåœ¨ top-k ç»“æœä¸Šï¼‰
    probs = torch.softmax(top_k_logits, dim=-1)
    cumulative_probs = torch.cumsum(probs, dim=-1)
    
    # ç§»é™¤ç´¯ç§¯æ¦‚ç‡è¶…è¿‡ p çš„
    indices_to_remove = cumulative_probs > top_p
    indices_to_remove[..., 0] = False
    top_k_logits[indices_to_remove] = float('-inf')
    
    # 4. é‡æ–°è®¡ç®—æ¦‚ç‡å¹¶é‡‡æ ·
    probs = torch.softmax(top_k_logits, dim=-1)
    sampled_index = torch.multinomial(probs, 1)
    next_token = top_k_indices.gather(-1, sampled_index)
    
    return next_token
```

### é‡‡æ ·ç­–ç•¥å¯¹æ¯”

| ç­–ç•¥ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| **è´ªå©ª** | ç¡®å®šæ€§ã€å¿«é€Ÿ | ç¼ºä¹å¤šæ ·æ€§ã€å¯èƒ½é‡å¤ | éœ€è¦ç¡®å®šæ€§è¾“å‡º |
| **Top-k** | æ§åˆ¶å¤šæ ·æ€§ã€ç®€å• | k å€¼éš¾ä»¥è°ƒæ•´ | ä¸€èˆ¬æ–‡æœ¬ç”Ÿæˆ |
| **Top-p** | è‡ªé€‚åº”ã€æ›´è‡ªç„¶ | è®¡ç®—ç¨æ…¢ | é«˜è´¨é‡æ–‡æœ¬ç”Ÿæˆ |
| **ç»„åˆ** | å¹³è¡¡æ€§èƒ½å’Œè´¨é‡ | æ›´å¤šè¶…å‚æ•° | ç”Ÿäº§ç¯å¢ƒ |

-----

## å®Œæ•´ç¤ºä¾‹ï¼šå¸¦å¤šç§é‡‡æ ·ç­–ç•¥çš„ç”Ÿæˆ

```python
def generate_with_options(
    model, 
    tokenizer, 
    prompt,
    max_length=50,
    method='greedy',  # 'greedy', 'top_k', 'top_p', 'combined'
    temperature=1.0,
    top_k=50,
    top_p=0.9
):
    """
    å®Œæ•´çš„ç”Ÿæˆå‡½æ•°ï¼Œæ”¯æŒå¤šç§é‡‡æ ·ç­–ç•¥
    """
    # ç¼–ç è¾“å…¥
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    
    # åˆ›å»º Static Cache
    past_key_values = StaticCache(
        config=model.config,
        max_batch_size=1,
        max_cache_len=max_length,
        device='jax',
        dtype=model.dtype
    )
    
    generated = input_ids[0].tolist()
    cache_position = torch.arange(input_ids.shape[1], device='jax')
    
    # ç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­
    with torch.no_grad():
        logits, past_key_values = model(
            input_ids,
            cache_position=cache_position,
            past_key_values=past_key_values,
            return_dict=False,
            use_cache=True
        )
    
    # é€‰æ‹©é‡‡æ ·æ–¹æ³•
    if method == 'greedy':
        next_token = torch.argmax(logits[:, -1], dim=-1)
    elif method == 'top_k':
        next_token = top_k_sampling(logits[:, -1], k=top_k, temperature=temperature)
    elif method == 'top_p':
        next_token = top_p_sampling(logits[:, -1], p=top_p, temperature=temperature)
    elif method == 'combined':
        next_token = combined_sampling(logits[:, -1], top_k=top_k, top_p=top_p, temperature=temperature)
    
    generated.append(next_token.item())
    
    # ç»§ç»­ç”Ÿæˆ
    cache_position = torch.tensor([input_ids.shape[1]], device='jax')
    for _ in range(max_length - input_ids.shape[1]):
        with torch.no_grad():
            logits, past_key_values = model(
                next_token.unsqueeze(0),
                cache_position=cache_position,
                past_key_values=past_key_values,
                return_dict=False,
                use_cache=True
            )
        
        # åº”ç”¨ç›¸åŒçš„é‡‡æ ·æ–¹æ³•
        if method == 'greedy':
            next_token = torch.argmax(logits[:, -1], dim=-1)
        elif method == 'top_k':
            next_token = top_k_sampling(logits[:, -1], k=top_k, temperature=temperature)
        elif method == 'top_p':
            next_token = top_p_sampling(logits[:, -1], p=top_p, temperature=temperature)
        elif method == 'combined':
            next_token = combined_sampling(logits[:, -1], top_k=top_k, top_p=top_p, temperature=temperature)
        
        if next_token.item() == tokenizer.eos_token_id:
            break
        
        generated.append(next_token.item())
        cache_position += 1
    
    # è§£ç 
    return tokenizer.decode(generated)

# ä½¿ç”¨ç¤ºä¾‹
prompt = "The secret to baking a good cake is"

print("è´ªå©ªé‡‡æ ·:")
print(generate_with_options(model, tokenizer, prompt, method='greedy'))

print("\nTop-k é‡‡æ ·:")
print(generate_with_options(model, tokenizer, prompt, method='top_k', temperature=0.8))

print("\nTop-p é‡‡æ ·:")
print(generate_with_options(model, tokenizer, prompt, method='top_p', temperature=0.8))

print("\nç»„åˆé‡‡æ ·:")
print(generate_with_options(model, tokenizer, prompt, method='combined', temperature=0.8))
```

-----

## å¸¸è§é—®é¢˜è§£ç­”

### Q: Static Cache çš„ max_cache_len åº”è¯¥è®¾ç½®ä¸ºå¤šå°‘ï¼Ÿ

A: è€ƒè™‘å› ç´ ï¼š
- **æœ€å¤§å¯èƒ½çš„åºåˆ—é•¿åº¦**ï¼šè®¾ç½®ä¸ºæ‚¨é¢„æœŸçš„æœ€å¤§å€¼
- **å†…å­˜é™åˆ¶**ï¼šæ›´å¤§çš„ç¼“å­˜æ¶ˆè€—æ›´å¤šå†…å­˜
- **å»ºè®®**ï¼šmax_input_length + max_new_tokens

### Q: ä¸ºä»€ä¹ˆéœ€è¦ torch.func.functional_callï¼Ÿ

A: åŸå› ï¼š
1. **é¿å…å¸¸é‡æ•è·**ï¼šæƒé‡ä½œä¸ºè¾“å…¥è€Œä¸æ˜¯é—­åŒ…å˜é‡
2. **æ— çŠ¶æ€è°ƒç”¨**ï¼šä¸ä¿®æ”¹åŸå§‹æ¨¡å‹
3. **JIT å‹å¥½**ï¼šæ˜ç¡®çš„æ•°æ®æµ

### Q: å¯ä»¥å¯¹æ•´ä¸ªç”Ÿæˆå¾ªç¯è¿›è¡Œ JIT å—ï¼Ÿ

A: æŠ€æœ¯ä¸Šå¯ä»¥ï¼Œä½†æœ‰æŒ‘æˆ˜ï¼š
- å¾ªç¯é•¿åº¦éœ€è¦å›ºå®š
- æå‰åœæ­¢ï¼ˆé‡åˆ° EOSï¼‰ä¼šå¾ˆå¤æ‚
- é€šå¸¸åª JIT å•æ­¥å‡½æ•°æ›´å®ç”¨

### Q: å¦‚ä½•å¤„ç†æ‰¹å¤„ç†ç”Ÿæˆï¼Ÿ

A: ä¿®æ”¹ä»£ç æ”¯æŒæ‰¹å¤„ç†ï¼š
```python
# åˆ›å»ºæ‰¹å¤„ç†ç¼“å­˜
past_key_values = StaticCache(
    max_batch_size=batch_size,  # å¢åŠ  batch size
    max_cache_len=max_length,
    ...
)

# å¤„ç†å¯å˜é•¿åº¦åºåˆ—
# ä½¿ç”¨æ³¨æ„åŠ›æ©ç å’Œå¡«å……
```

### Q: æ€§èƒ½ç“¶é¢ˆåœ¨å“ªé‡Œï¼Ÿ

A: å¸¸è§ç“¶é¢ˆï¼š
1. **ç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­**ï¼šå¤„ç†æ•´ä¸ªè¾“å…¥åºåˆ—
2. **å¤§å‹ KV Cache**ï¼šå†…å­˜å¸¦å®½
3. **é‡‡æ ·**ï¼šå¤æ‚çš„é‡‡æ ·ç­–ç•¥å¯èƒ½è¾ƒæ…¢

-----

## æ€»ç»“

æœ¬æ–‡å±•ç¤ºäº†ï¼š

### å…³é”®æˆå°±

âœ… **ç†è§£ torchax çš„å·¥ä½œåŸç†**ï¼šJAX æ•°ç»„ä¼ªè£…æˆ PyTorch Tensor
âœ… **å®ç°è‡ªå›å½’è§£ç **ï¼šä½¿ç”¨ KV Cache ä¼˜åŒ–
âœ… **Static Cache**ï¼šé¿å…å½¢çŠ¶å˜åŒ–ï¼Œæ”¯æŒ JIT
âœ… **æˆåŠŸ JIT ç¼–è¯‘**ï¼š8.8x åŠ é€Ÿï¼ˆç›¸æ¯” Dynamic Cacheï¼‰
âœ… **é¿å…å¸¸é‡æ•è·**ï¼šä½¿ç”¨ `torch.func.functional_call`

### æ€§èƒ½æ€»ç»“

| æ–¹æ³• | æ—¶é—´ï¼ˆç§’ï¼‰ | åŠ é€Ÿæ¯” |
|------|-----------|--------|
| Dynamic Cache (eager) | 130.9 | 1x |
| Static Cache (eager) | 88.4 | 1.48x |
| **Static Cache (JIT)** | **14.8** | **8.8x** |

### å…³é”®æŠ€æœ¯

1. **Pytree æ³¨å†Œ**ï¼šæ”¯æŒè‡ªå®šä¹‰ç±»å‹
2. **Static Cache**ï¼šå›ºå®šå½¢çŠ¶ï¼Œæ”¯æŒ JIT
3. **functional_call**ï¼šé¿å…å¸¸é‡æ•è·
4. **jax_jit**ï¼šç¼–è¯‘ PyTorch å‡½æ•°

### ä¸‹ä¸€æ­¥

åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†å¯¹ HuggingFace diffusers åº“ä¸­çš„æ¨¡å‹åšåŒæ ·çš„äº‹æƒ…ï¼Œå±•ç¤ºè¿™ç§æ–¹æ³•å¯¹å›¾åƒç”Ÿæˆæ¨¡å‹çš„é€‚ç”¨æ€§ã€‚

æ•¬è¯·æœŸå¾…ï¼